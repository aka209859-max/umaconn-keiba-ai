#!/usr/bin/env python3
"""
PCkeibaãƒ¬ãƒ¼ã‚¹çµæœãƒ‡ãƒ¼ã‚¿ã®åŒ…æ‹¬çš„åˆ†æã‚¹ã‚¯ãƒªãƒ—ãƒˆ

ç›®çš„:
1) æ é †ä¿‚æ•°ã®é«˜ç²¾åº¦åŒ–ï¼ˆGLM/æ©Ÿæ¢°å­¦ç¿’ï¼‰
2) ä¸ŠãŒã‚ŠæŒ‡æ•°ã®ãƒšãƒ¼ã‚¹è£œæ­£å€¤æ¤œè¨¼
3) 4ã¤ã®æŒ‡æ•°ã®ç«¶é¦¬å ´åˆ¥ä¸Šé™ä¸‹é™ç¢ºèª

ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹: PCkeibaå…¨æœŸé–“ãƒ‡ãƒ¼ã‚¿ï¼ˆç´„320ä¸‡é ­ï¼‰
ä½œæˆæ—¥: 2026-01-10
ä½œæˆè€…: NAR-AI-YOSO Project
"""

import pandas as pd
import numpy as np
from pathlib import Path
from collections import defaultdict
import json

# ============================
# è¨­å®š
# ============================
INPUT_CSV = '/home/user/uploaded_files/data-1768047611955.csv'
OUTPUT_DIR = Path('/home/user/webapp/nar-ai-yoso/data')
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

# ============================
# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
# ============================
print('ğŸ“‚ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­...')
print(f'ãƒ•ã‚¡ã‚¤ãƒ«: {INPUT_CSV}')

# ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°èª­ã¿è¾¼ã¿ï¼ˆãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–ï¼‰
# å…¨ãƒ‡ãƒ¼ã‚¿ï¼ˆ320ä¸‡è¡Œï¼‰ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆ10%ï¼32ä¸‡è¡Œï¼‰
SAMPLE_RATE = 0.1
print(f'  ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆ: {SAMPLE_RATE * 100:.0f}%')

df = pd.read_csv(INPUT_CSV, low_memory=False, 
                 skiprows=lambda i: i > 0 and np.random.rand() > SAMPLE_RATE)
print(f'\nâœ… ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(df):,} è¡Œï¼ˆã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ¸ˆã¿ï¼‰')

# ============================
# ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°
# ============================
print('\nğŸ§¹ ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°ä¸­...')

# å¿…é ˆã‚«ãƒ©ãƒ ã®ç¢ºèª
required_cols = ['race_id', 'keibajo_code', 'kyori', 'wakuban', 'chakujun', 
                 'tosu', 'soha_time_sec', 'kohan_3f_sec', 'tansho_flag', 'fukusho_flag']
missing_cols = [col for col in required_cols if col not in df.columns]
if missing_cols:
    raise ValueError(f'âŒ å¿…é ˆã‚«ãƒ©ãƒ ãŒä¸è¶³: {missing_cols}')

# æ•°å€¤å‹ã¸ã®å¤‰æ›
df['keibajo_code'] = pd.to_numeric(df['keibajo_code'], errors='coerce')
df['kyori'] = pd.to_numeric(df['kyori'], errors='coerce')
df['wakuban'] = pd.to_numeric(df['wakuban'], errors='coerce')
df['chakujun'] = pd.to_numeric(df['chakujun'], errors='coerce')
df['tosu'] = pd.to_numeric(df['tosu'], errors='coerce')
df['soha_time_sec'] = pd.to_numeric(df['soha_time_sec'], errors='coerce')
df['kohan_3f_sec'] = pd.to_numeric(df['kohan_3f_sec'], errors='coerce')
df['tansho_flag'] = pd.to_numeric(df['tansho_flag'], errors='coerce')
df['fukusho_flag'] = pd.to_numeric(df['fukusho_flag'], errors='coerce')

# ã‚³ãƒ¼ãƒŠãƒ¼é †ä½ã®å¤‰æ›
for i in range(1, 5):
    col = f'corner_{i}'
    if col in df.columns:
        df[col] = pd.to_numeric(df[col], errors='coerce')

# ç•°å¸¸å€¤é™¤å¤–
df = df[df['wakuban'].between(1, 8)]
df = df[df['kyori'].between(800, 3000)]
df = df[df['tosu'] >= 3]
df = df[df['soha_time_sec'] > 0]
df = df[df['kohan_3f_sec'] > 0]

print(f'âœ… ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°å®Œäº†: {len(df):,} è¡Œï¼ˆæœ‰åŠ¹ãƒ‡ãƒ¼ã‚¿ï¼‰')

# ============================
# åŸºæœ¬çµ±è¨ˆé‡
# ============================
print('\nğŸ“Š åŸºæœ¬çµ±è¨ˆé‡:')
print(f'  ç«¶é¦¬å ´æ•°: {df["keibajo_code"].nunique()}')
print(f'  è·é›¢ç¨®é¡: {df["kyori"].nunique()}')
print(f'  æœŸé–“: {df["race_date"].min()} ~ {df["race_date"].max()}')
print(f'  å¹³å‡å‡ºèµ°é ­æ•°: {df["tosu"].mean():.1f}é ­')

# ç«¶é¦¬å ´åˆ¥ãƒ¬ãƒ¼ã‚¹æ•°
keibajo_counts = df.groupby('keibajo_code').size().sort_values(ascending=False)
print(f'\nğŸ“ ç«¶é¦¬å ´åˆ¥ãƒ¬ãƒ¼ã‚¹æ•°ï¼ˆTop 10ï¼‰:')
for keibajo_code, count in keibajo_counts.head(10).items():
    keibajo_name = df[df['keibajo_code'] == keibajo_code]['keibajo_name'].iloc[0]
    print(f'  {keibajo_name} ({keibajo_code}): {count:,}é ­')

# ============================
# ã‚¿ã‚¹ã‚¯1: æ é †ä¿‚æ•°ã®é«˜ç²¾åº¦å†ç®—å‡º
# ============================
print('\n' + '='*60)
print('ã‚¿ã‚¹ã‚¯1: æ é †ä¿‚æ•°ã®é«˜ç²¾åº¦å†ç®—å‡ºï¼ˆGLMæº–å‚™ï¼‰')
print('='*60)

# æ é †åˆ¥ã®çš„ä¸­ç‡ã‚’ç«¶é¦¬å ´Ã—è·é›¢åˆ¥ã«é›†è¨ˆ
wakuban_stats = df.groupby(['keibajo_code', 'kyori', 'wakuban']).agg({
    'race_id': 'count',  # ã‚µãƒ³ãƒ—ãƒ«æ•°
    'tansho_flag': 'sum',  # å˜å‹çš„ä¸­æ•°
    'fukusho_flag': 'sum'  # è¤‡å‹çš„ä¸­æ•°
}).reset_index()

wakuban_stats.columns = ['keibajo_code', 'kyori', 'wakuban', 'sample_count', 
                         'tansho_hit_count', 'fukusho_hit_count']

# çš„ä¸­ç‡ã®è¨ˆç®—
wakuban_stats['tansho_hit_rate'] = (wakuban_stats['tansho_hit_count'] / 
                                    wakuban_stats['sample_count'] * 100).round(2)
wakuban_stats['fukusho_hit_rate'] = (wakuban_stats['fukusho_hit_count'] / 
                                     wakuban_stats['sample_count'] * 100).round(2)

# ç«¶é¦¬å ´åã‚’ãƒãƒ¼ã‚¸
keibajo_names = df[['keibajo_code', 'keibajo_name']].drop_duplicates()
wakuban_stats = wakuban_stats.merge(keibajo_names, on='keibajo_code', how='left')

# å‡ºåŠ›
output_csv = OUTPUT_DIR / 'pckeiba_wakuban_stats.csv'
wakuban_stats.to_csv(output_csv, index=False)
print(f'âœ… æ é †åˆ¥çµ±è¨ˆé‡ã‚’ä¿å­˜: {output_csv}')
print(f'  ãƒ‡ãƒ¼ã‚¿æ•°: {len(wakuban_stats):,} è¡Œ')

# ============================
# ã‚¿ã‚¹ã‚¯2: ãƒšãƒ¼ã‚¹è£œæ­£å€¤ã®æ¤œè¨¼
# ============================
print('\n' + '='*60)
print('ã‚¿ã‚¹ã‚¯2: ãƒšãƒ¼ã‚¹è£œæ­£å€¤ã®æ¤œè¨¼ï¼ˆå‰åŠ3Fæ¨å®šï¼‰')
print('='*60)

# å‰åŠ3Fã‚¿ã‚¤ãƒ ã®æ¨å®šï¼ˆ1200mä»¥ä¸‹ã¯direct_calculationï¼‰
df['zenhan_3f_estimated'] = df['soha_time_sec'] - df['kohan_3f_sec']

# ãƒšãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—ã®åˆ†é¡ï¼ˆç°¡æ˜“ç‰ˆï¼šå…¨ä½“å¹³å‡ã¨ã®æ¯”è¼ƒï¼‰
kyori_zenhan_mean = df.groupby('kyori')['zenhan_3f_estimated'].mean().to_dict()
df['zenhan_3f_mean'] = df['kyori'].map(kyori_zenhan_mean)
df['pace_deviation'] = df['zenhan_3f_estimated'] - df['zenhan_3f_mean']

# ãƒšãƒ¼ã‚¹åˆ†é¡
def classify_pace(deviation):
    if pd.isna(deviation):
        return 'M'
    elif deviation < -0.8:  # å‰åŠãŒé€Ÿã„ï¼ˆãƒã‚¤ãƒšãƒ¼ã‚¹ï¼‰
        return 'H'
    elif deviation > 0.8:   # å‰åŠãŒé…ã„ï¼ˆã‚¹ãƒ­ãƒ¼ãƒšãƒ¼ã‚¹ï¼‰
        return 'S'
    else:
        return 'M'

df['pace_type'] = df['pace_deviation'].apply(classify_pace)

# ãƒšãƒ¼ã‚¹åˆ¥ã®å¾ŒåŠ3Få¹³å‡
pace_stats = df.groupby('pace_type').agg({
    'kohan_3f_sec': ['mean', 'std', 'count']
}).round(2)

print('ğŸ“Š ãƒšãƒ¼ã‚¹åˆ¥ã®å¾ŒåŠ3Fçµ±è¨ˆé‡:')
print(pace_stats)

# ãƒšãƒ¼ã‚¹è£œæ­£å€¤ã®å¦¥å½“æ€§æ¤œè¨¼
pace_correction_verification = df.groupby('pace_type').agg({
    'kohan_3f_sec': 'mean',
    'soha_time_sec': 'mean',
    'race_id': 'count'
}).round(2)

pace_correction_verification.columns = ['å¾ŒåŠ3Få¹³å‡', 'èµ°ç ´ã‚¿ã‚¤ãƒ å¹³å‡', 'ã‚µãƒ³ãƒ—ãƒ«æ•°']
print('\nğŸ“Š ãƒšãƒ¼ã‚¹è£œæ­£å€¤ã®å¦¥å½“æ€§æ¤œè¨¼:')
print(pace_correction_verification)

# åŸºæº–å€¤ï¼ˆMãƒšãƒ¼ã‚¹ï¼‰ã¨ã®å·®åˆ†
m_kohan_3f = pace_correction_verification.loc['M', 'å¾ŒåŠ3Få¹³å‡']
pace_correction_verification['å¾ŒåŠ3Få·®åˆ†'] = (
    pace_correction_verification['å¾ŒåŠ3Få¹³å‡'] - m_kohan_3f
).round(2)

print('\nâœ… ãƒšãƒ¼ã‚¹åˆ¥ã®å¾ŒåŠ3Få·®åˆ†ï¼ˆåŸºæº–å€¤Mã¨ã®æ¯”è¼ƒï¼‰:')
print(pace_correction_verification[['å¾ŒåŠ3Få·®åˆ†', 'ã‚µãƒ³ãƒ—ãƒ«æ•°']])

# å‡ºåŠ›
output_csv = OUTPUT_DIR / 'pckeiba_pace_verification.csv'
pace_correction_verification.to_csv(output_csv)
print(f'\nâœ… ãƒšãƒ¼ã‚¹æ¤œè¨¼çµæœã‚’ä¿å­˜: {output_csv}')

# ============================
# ã‚¿ã‚¹ã‚¯3: 4ã¤ã®æŒ‡æ•°ã®ç«¶é¦¬å ´åˆ¥ä¸Šé™ä¸‹é™
# ============================
print('\n' + '='*60)
print('ã‚¿ã‚¹ã‚¯3: 4ã¤ã®æŒ‡æ•°ã®ç«¶é¦¬å ´åˆ¥ä¸Šé™ä¸‹é™')
print('='*60)

# æ³¨: å®Ÿéš›ã®æŒ‡æ•°è¨ˆç®—ã«ã¯ core/index_calculator.py ãŒå¿…è¦
# ã“ã“ã§ã¯åŸºç¤ãƒ‡ãƒ¼ã‚¿ã®çµ±è¨ˆé‡ã®ã¿ã‚’ç®—å‡º

index_ranges = df.groupby('keibajo_code').agg({
    'zenhan_3f_estimated': ['min', 'max', 'mean', 'std'],  # ãƒ†ãƒ³æŒ‡æ•°ã®åŸºç¤
    'kohan_3f_sec': ['min', 'max', 'mean', 'std'],         # ä¸ŠãŒã‚ŠæŒ‡æ•°ã®åŸºç¤
    'soha_time_sec': ['min', 'max', 'mean', 'std'],        # å…¨ä½“ã‚¿ã‚¤ãƒ 
    'kyori': ['min', 'max', 'mean']                         # è·é›¢
}).round(2)

print('ğŸ“Š ç«¶é¦¬å ´åˆ¥ã®åŸºç¤çµ±è¨ˆé‡:')
print(index_ranges.head(10))

# å‡ºåŠ›
output_csv = OUTPUT_DIR / 'pckeiba_index_ranges.csv'
index_ranges.to_csv(output_csv)
print(f'\nâœ… æŒ‡æ•°ç¯„å›²çµ±è¨ˆé‡ã‚’ä¿å­˜: {output_csv}')

# ============================
# ã‚µãƒãƒªãƒ¼
# ============================
print('\n' + '='*60)
print('ğŸ“Š åˆ†æå®Œäº†ã‚µãƒãƒªãƒ¼')
print('='*60)
print(f'âœ… ã‚¿ã‚¹ã‚¯1: æ é †åˆ¥çµ±è¨ˆé‡ â†’ {OUTPUT_DIR / "pckeiba_wakuban_stats.csv"}')
print(f'âœ… ã‚¿ã‚¹ã‚¯2: ãƒšãƒ¼ã‚¹æ¤œè¨¼çµæœ â†’ {OUTPUT_DIR / "pckeiba_pace_verification.csv"}')
print(f'âœ… ã‚¿ã‚¹ã‚¯3: æŒ‡æ•°ç¯„å›²çµ±è¨ˆé‡ â†’ {OUTPUT_DIR / "pckeiba_index_ranges.csv"}')
print('\nğŸ¯ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:')
print('  1) GLMã«ã‚ˆã‚‹æ é †ä¿‚æ•°ã®é«˜ç²¾åº¦åŒ–')
print('  2) ãƒšãƒ¼ã‚¹è£œæ­£å€¤ã®æœ€çµ‚æ¤œè¨¼')
print('  3) ãƒ™ã‚¤ã‚ºæ¨å®šä¿‚æ•°ã¨ã®æ¯”è¼ƒ')
print('\nâœ… Phase 3 å®Œäº†ï¼')
