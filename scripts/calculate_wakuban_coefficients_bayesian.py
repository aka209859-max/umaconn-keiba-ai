#!/usr/bin/env python3
"""
æ é †ä¿‚æ•°ã®ãƒ™ã‚¤ã‚ºç¸®å°æ¨å®šï¼ˆBayesian Shrinkage Estimationï¼‰

ç›®çš„:
- ã‚µãƒ³ãƒ—ãƒ«æ•°ã‚’è€ƒæ…®ã—ãŸçµ±è¨ˆçš„ã«ä¿¡é ¼æ€§ã®é«˜ã„æ é †ä¿‚æ•°ã‚’ç®—å‡º
- éå‰°é©åˆï¼ˆOverfittingï¼‰ã‚’é˜²æ­¢
- ä¿¡é ¼åŒºé–“ï¼ˆ95% CIï¼‰ä»˜ãæ¨å®š

ç†è«–çš„èƒŒæ™¯:
- Betaåˆ†å¸ƒã«ã‚ˆã‚‹äº‹å¾Œæ¨å®š
- James-Steinæ¨å®šé‡ã«ã‚ˆã‚‹ç¸®å°æ¨å®š
- ã‚µãƒ³ãƒ—ãƒ«æ•°ãŒå°‘ãªã„å ´åˆã€å…¨ä½“å¹³å‡ã«è¿‘ã¥ã‘ã‚‹

Author: NAR-AI-YOSO Project
Date: 2026-01-10
"""

import pandas as pd
import numpy as np
from scipy.stats import beta
import json
from pathlib import Path
from collections import defaultdict

# ============================
# è¨­å®š
# ============================

INPUT_CSV = '/home/user/uploaded_files/data-1768033229370.csv'
OUTPUT_CSV = '/home/user/webapp/nar-ai-yoso/data/wakuban_coefficients_bayesian.csv'
OUTPUT_JSON = '/home/user/webapp/nar-ai-yoso/data/wakuban_coefficients_bayesian.json'

# ã‚¹ã‚±ãƒ¼ãƒ«ä¿‚æ•°ï¼ˆä½ç½®æŒ‡æ•°ã¸ã®å½±éŸ¿åº¦ï¼‰
SCALE_FACTOR = 1.5  # 1%ã®çš„ä¸­ç‡å·® = 1.5ç‚¹ã®æŒ‡æ•°å·®

# ãƒ™ã‚¤ã‚ºæ¨å®šã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
OVERALL_HIT_RATE = 0.10  # å…¨ä½“å¹³å‡10%ã¨ä»®å®š
ALPHA_PRIOR = OVERALL_HIT_RATE * 100  # Î± = 10
BETA_PRIOR = (1 - OVERALL_HIT_RATE) * 100  # Î² = 90

# çµ±è¨ˆçš„æœ‰æ„æ€§ã®é–¾å€¤
SIGNIFICANCE_LEVEL = 0.05  # 5%æœ‰æ„æ°´æº–

# ============================
# ãƒ™ã‚¤ã‚ºæ¨å®šé–¢æ•°
# ============================

def bayesian_shrinkage(k, n, alpha_prior, beta_prior):
    """
    ãƒ™ã‚¤ã‚ºç¸®å°æ¨å®šã«ã‚ˆã‚‹çš„ä¸­ç‡ã®äº‹å¾ŒæœŸå¾…å€¤
    
    Args:
        k: çš„ä¸­æ•°ï¼ˆsuccessesï¼‰
        n: ã‚µãƒ³ãƒ—ãƒ«æ•°ï¼ˆtrialsï¼‰
        alpha_prior: äº‹å‰åˆ†å¸ƒã®Î±ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        beta_prior: äº‹å‰åˆ†å¸ƒã®Î²ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    
    Returns:
        posterior_mean: äº‹å¾ŒæœŸå¾…å€¤ï¼ˆçœŸã®çš„ä¸­ç‡ã®æ¨å®šå€¤ï¼‰
        ci_lower: 95%ä¿¡é ¼åŒºé–“ã®ä¸‹é™
        ci_upper: 95%ä¿¡é ¼åŒºé–“ã®ä¸Šé™
        shrinkage_rate: ç¸®å°ç‡ï¼ˆ0%ã€œ100%ï¼‰
    """
    # äº‹å¾Œåˆ†å¸ƒã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    alpha_post = alpha_prior + k
    beta_post = beta_prior + (n - k)
    
    # äº‹å¾ŒæœŸå¾…å€¤ï¼ˆç¸®å°æ¨å®šï¼‰
    posterior_mean = alpha_post / (alpha_post + beta_post)
    
    # 95%ä¿¡é ¼åŒºé–“
    ci_lower = beta.ppf(0.025, alpha_post, beta_post)
    ci_upper = beta.ppf(0.975, alpha_post, beta_post)
    
    # ç¸®å°ç‡ã®è¨ˆç®—
    # ã‚µãƒ³ãƒ—ãƒ«æ•°ãŒå¤šã„ã»ã©è¦³æ¸¬å€¤ã«è¿‘ã¥ãï¼ˆç¸®å°ç‡0%ï¼‰
    # ã‚µãƒ³ãƒ—ãƒ«æ•°ãŒå°‘ãªã„ã»ã©äº‹å‰åˆ†å¸ƒã«è¿‘ã¥ãï¼ˆç¸®å°ç‡100%ï¼‰
    mle = k / n if n > 0 else 0.5  # æœ€å°¤æ¨å®šï¼ˆMaximum Likelihood Estimationï¼‰
    prior_mean = alpha_prior / (alpha_prior + beta_prior)
    
    if mle == posterior_mean:
        shrinkage_rate = 0.0
    else:
        shrinkage_rate = abs(posterior_mean - mle) / abs(prior_mean - mle) * 100 if mle != prior_mean else 100.0
    
    return posterior_mean, ci_lower, ci_upper, shrinkage_rate

# ============================
# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
# ============================

print("ğŸ“‚ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­...")
df = pd.read_csv(INPUT_CSV)

print(f"âœ… ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(df)} è¡Œ")
print(f"   ç«¶é¦¬å ´æ•°: {df['keibajo_code'].nunique()}")
print(f"   è·é›¢æ•°: {df['distance'].nunique()}")
print(f"   æ ç•ªæ•°: {df['waku'].nunique()}")

# ============================
# å¹³å‡çš„ä¸­ç‡ã®è¨ˆç®—
# ============================

print("\nğŸ“Š å¹³å‡çš„ä¸­ç‡ã‚’è¨ˆç®—ä¸­...")

# å˜å‹ã¨è¤‡å‹ã®å¹³å‡çš„ä¸­ç‡
df['avg_hit_rate'] = (df['tansho_hit_rate'] + df['fukusho_hit_rate']) / 2

print(f"âœ… å¹³å‡çš„ä¸­ç‡ã®è¨ˆç®—å®Œäº†")
print(f"   æœ€å°: {df['avg_hit_rate'].min():.2f}%")
print(f"   æœ€å¤§: {df['avg_hit_rate'].max():.2f}%")
print(f"   å¹³å‡: {df['avg_hit_rate'].mean():.2f}%")

# ============================
# ç«¶é¦¬å ´Ã—è·é›¢åˆ¥ã®åŸºæº–å€¤ã‚’è¨ˆç®—
# ============================

print("\nğŸ¯ åŸºæº–å€¤ï¼ˆå„ç«¶é¦¬å ´Ã—è·é›¢ã®å…¨æ å¹³å‡ï¼‰ã‚’è¨ˆç®—ä¸­...")

baseline = df.groupby(['keibajo_code', 'distance'])['avg_hit_rate'].mean().reset_index()
baseline.columns = ['keibajo_code', 'distance', 'baseline_rate']

print(f"âœ… åŸºæº–å€¤ã®è¨ˆç®—å®Œäº†: {len(baseline)} çµ„ã¿åˆã‚ã›")

# ãƒ‡ãƒ¼ã‚¿ã‚’ãƒãƒ¼ã‚¸
df = df.merge(baseline, on=['keibajo_code', 'distance'], how='left')

# ============================
# ãƒ™ã‚¤ã‚ºæ¨å®šã«ã‚ˆã‚‹æ é †ä¿‚æ•°ã®è¨ˆç®—
# ============================

print("\nğŸ”¬ ãƒ™ã‚¤ã‚ºæ¨å®šã‚’å®Ÿè¡Œä¸­...")

# çµæœã‚’æ ¼ç´ã™ã‚‹ãƒªã‚¹ãƒˆ
results = []

for idx, row in df.iterrows():
    # çš„ä¸­æ•°ã¨ã‚µãƒ³ãƒ—ãƒ«æ•°ã‚’è¨ˆç®—
    avg_hit_rate_decimal = row['avg_hit_rate'] / 100
    k = row['sample_count'] * avg_hit_rate_decimal  # çš„ä¸­æ•°
    n = row['sample_count']  # ã‚µãƒ³ãƒ—ãƒ«æ•°
    
    # ãƒ™ã‚¤ã‚ºæ¨å®š
    posterior_mean, ci_lower, ci_upper, shrinkage_rate = bayesian_shrinkage(
        k, n, ALPHA_PRIOR, BETA_PRIOR
    )
    
    # åŸºæº–å€¤ã¨ã®å·®åˆ†ã‚’ä¿‚æ•°åŒ–
    baseline = row['baseline_rate'] / 100
    coefficient = (posterior_mean - baseline) * SCALE_FACTOR * 100
    
    # çµ±è¨ˆçš„æœ‰æ„æ€§ã®åˆ¤å®š
    # åŸºæº–å€¤ãŒ95%ä¿¡é ¼åŒºé–“ã«å«ã¾ã‚Œã‚‹å ´åˆã€çµ±è¨ˆçš„ã«æœ‰æ„ã§ãªã„ãŸã‚ä¿‚æ•°ã‚’0ã«
    is_significant = not (ci_lower < baseline < ci_upper)
    
    if not is_significant:
        coefficient = 0.0
    
    # çµæœã‚’æ ¼ç´
    results.append({
        'keibajo_code': row['keibajo_code'],
        'keibajo_name': row['keibajo_name'],
        'distance': row['distance'],
        'waku': row['waku'],
        'sample_count': row['sample_count'],
        'tansho_hit_rate': row['tansho_hit_rate'],
        'fukusho_hit_rate': row['fukusho_hit_rate'],
        'avg_hit_rate': row['avg_hit_rate'],
        'baseline_rate': row['baseline_rate'],
        'mle_hit_rate': (k / n * 100) if n > 0 else 0,  # æœ€å°¤æ¨å®š
        'bayesian_hit_rate': posterior_mean * 100,  # ãƒ™ã‚¤ã‚ºæ¨å®š
        'ci_lower': ci_lower * 100,
        'ci_upper': ci_upper * 100,
        'shrinkage_rate': shrinkage_rate,
        'is_significant': is_significant,
        'wakuban_coefficient': coefficient
    })

# DataFrameã«å¤‰æ›
df_bayesian = pd.DataFrame(results)

print(f"âœ… ãƒ™ã‚¤ã‚ºæ¨å®šã®å®Œäº†")
print(f"   æœ€å°ä¿‚æ•°: {df_bayesian['wakuban_coefficient'].min():.2f}")
print(f"   æœ€å¤§ä¿‚æ•°: {df_bayesian['wakuban_coefficient'].max():.2f}")
print(f"   å¹³å‡ä¿‚æ•°: {df_bayesian['wakuban_coefficient'].mean():.2f}")
print(f"   çµ±è¨ˆçš„æœ‰æ„ãªä¿‚æ•°ã®æ•°: {df_bayesian['is_significant'].sum()} / {len(df_bayesian)}")

# ============================
# CSVå‡ºåŠ›
# ============================

print(f"\nğŸ’¾ CSVå‡ºåŠ›ä¸­: {OUTPUT_CSV}")

# ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
Path(OUTPUT_CSV).parent.mkdir(parents=True, exist_ok=True)

df_bayesian.to_csv(OUTPUT_CSV, index=False, encoding='utf-8-sig')
print(f"âœ… CSVå‡ºåŠ›å®Œäº†: {len(df_bayesian)} è¡Œ")

# ============================
# JSONå‡ºåŠ›ï¼ˆãƒ—ãƒ­ã‚°ãƒ©ãƒ ç”¨ï¼‰
# ============================

print(f"\nğŸ’¾ JSONå‡ºåŠ›ä¸­: {OUTPUT_JSON}")

# æ§‹é€ : {keibajo_code: {kyori: {wakuban: coefficient}}}
coefficients = defaultdict(lambda: defaultdict(dict))

for _, row in df_bayesian.iterrows():
    keibajo = str(int(row['keibajo_code']))
    kyori = int(row['distance'])
    waku = int(row['waku'])
    coef = round(float(row['wakuban_coefficient']), 2)
    
    coefficients[keibajo][kyori][waku] = coef

# JSONæ›¸ãè¾¼ã¿
with open(OUTPUT_JSON, 'w', encoding='utf-8') as f:
    json.dump(coefficients, f, ensure_ascii=False, indent=2)

print(f"âœ… JSONå‡ºåŠ›å®Œäº†")

# ============================
# çµ±è¨ˆã‚µãƒãƒªãƒ¼
# ============================

print("\n" + "="*60)
print("ğŸ“Š ãƒ™ã‚¤ã‚ºæ¨å®šã®çµ±è¨ˆã‚µãƒãƒªãƒ¼ï¼ˆç¾è¡Œæ–¹å¼ã¨ã®æ¯”è¼ƒï¼‰")
print("="*60)

# å…ƒã®ãƒ‡ãƒ¼ã‚¿ï¼ˆç¾è¡Œæ–¹å¼ï¼‰ã‚’èª­ã¿è¾¼ã¿
df_old = pd.read_csv('/home/user/webapp/nar-ai-yoso/data/wakuban_coefficients.csv')

# æ¯”è¼ƒ
comparison = pd.DataFrame({
    'æŒ‡æ¨™': ['æœ€å°ä¿‚æ•°', 'æœ€å¤§ä¿‚æ•°', 'å¹³å‡ä¿‚æ•°', 'æ¨™æº–åå·®', 'æ¥µç«¯ãªä¿‚æ•°(|ä¿‚æ•°|>10)ã®æ•°', 'ä¿‚æ•°=0ã®æ•°'],
    'ç¾è¡Œæ–¹å¼': [
        df_old['wakuban_coefficient'].min(),
        df_old['wakuban_coefficient'].max(),
        df_old['wakuban_coefficient'].mean(),
        df_old['wakuban_coefficient'].std(),
        (df_old['wakuban_coefficient'].abs() > 10).sum(),
        (df_old['wakuban_coefficient'] == 0).sum()
    ],
    'ãƒ™ã‚¤ã‚ºæ¨å®š': [
        df_bayesian['wakuban_coefficient'].min(),
        df_bayesian['wakuban_coefficient'].max(),
        df_bayesian['wakuban_coefficient'].mean(),
        df_bayesian['wakuban_coefficient'].std(),
        (df_bayesian['wakuban_coefficient'].abs() > 10).sum(),
        (df_bayesian['wakuban_coefficient'] == 0).sum()
    ]
})

print(comparison.to_string(index=False))

print("\n" + "="*60)
print("ğŸ” ã‚µãƒ³ãƒ—ãƒ«æ•°ã«ã‚ˆã‚‹ç¸®å°ç‡ã®çµ±è¨ˆ")
print("="*60)

# ã‚µãƒ³ãƒ—ãƒ«æ•°ã§ãƒ“ãƒ³åˆ†ã‘
df_bayesian['sample_bin'] = pd.cut(
    df_bayesian['sample_count'], 
    bins=[0, 100, 500, 1000, 5000, 10000],
    labels=['<100', '100-500', '500-1000', '1000-5000', '5000+']
)

shrinkage_summary = df_bayesian.groupby('sample_bin').agg({
    'shrinkage_rate': ['mean', 'std'],
    'sample_count': 'count'
}).round(2)

print(shrinkage_summary)

print("\n" + "="*60)
print("âœ… å®Œäº†ï¼")
print("="*60)
print(f"ğŸ“ å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«:")
print(f"   - {OUTPUT_CSV}")
print(f"   - {OUTPUT_JSON}")
print("="*60)
