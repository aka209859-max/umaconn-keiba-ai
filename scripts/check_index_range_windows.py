#!/usr/bin/env python3
"""
å„æŒ‡æ•°ã®å€¤ã®ç¯„å›²ç¢ºèªã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆWindowsç”¨ï¼‰

ã€ç›®çš„ã€‘
NAR-SI3.0 ã®å„æŒ‡æ•°ï¼ˆä¸ŠãŒã‚ŠæŒ‡æ•°ã€ä½ç½®æŒ‡æ•°ã€ãƒ†ãƒ³æŒ‡æ•°ã€ãƒšãƒ¼ã‚¹æŒ‡æ•°ï¼‰ã®
æœ€å°å€¤ã€æœ€å¤§å€¤ã€å¹³å‡å€¤ã€ä¸­å¤®å€¤ã€åˆ†å¸ƒã‚’ç¢ºèªã™ã‚‹

ã€ä½¿ç”¨æ–¹æ³•ã€‘
python check_index_range_windows.py

ã€å‡ºåŠ›ã€‘
- å„æŒ‡æ•°ã®çµ±è¨ˆæƒ…å ±ï¼ˆæœ€å°å€¤ã€æœ€å¤§å€¤ã€å¹³å‡ã€ä¸­å¤®å€¤ã€æ¨™æº–åå·®ï¼‰
- åˆ†ä½ç‚¹ï¼ˆ5%, 25%, 50%, 75%, 95%ï¼‰
- ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ æƒ…å ±
"""

import os
import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings('ignore')

# ============================
# è¨­å®š
# ============================

# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š
DEFAULT_DATA_PATH = r'E:\UmaData\nar-analytics-python-v2\data-1768047611955.csv'
DEFAULT_START_DATE = '20231013'
DEFAULT_END_DATE = '20251231'
DEFAULT_SAMPLE_RATE = 0.1

# ============================
# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
# ============================

def load_and_filter_data(file_path: str, start_date: str, end_date: str, sample_rate: float = 0.1):
    """ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°"""
    print(f"ğŸ“ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿: {file_path}")
    
    # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°èª­ã¿è¾¼ã¿
    df = pd.read_csv(file_path, skiprows=lambda i: i > 0 and np.random.rand() > sample_rate)
    print(f"   èª­ã¿è¾¼ã¿: {len(df):,}è¡Œï¼ˆã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°{int(sample_rate*100)}%ï¼‰")
    
    # æœŸé–“ãƒ•ã‚£ãƒ«ã‚¿
    df['race_date'] = df['race_date'].astype(str)
    df = df[(df['race_date'] >= start_date) & (df['race_date'] <= end_date)]
    print(f"   æœŸé–“ãƒ•ã‚£ãƒ«ã‚¿: {len(df):,}è¡Œï¼ˆ{start_date}ï½{end_date}ï¼‰")
    
    # ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°
    df = df.dropna(subset=['race_date', 'keibajo_code', 'kyori', 'wakuban', 'chakujun', 
                            'soha_time_sec', 'kohan_3f_sec', 'weight_kg', 'tosu'])
    df = df[df['soha_time_sec'] > 0]
    df = df[df['kohan_3f_sec'] > 0]
    df = df[df['tosu'] >= 4]
    print(f"   ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°å¾Œ: {len(df):,}è¡Œ\n")
    
    return df


# ============================
# å„æŒ‡æ•°è¨ˆç®—ï¼ˆç°¡æ˜“ç‰ˆï¼‰
# ============================

def calculate_all_indices_simple(df: pd.DataFrame) -> pd.DataFrame:
    """NAR-SI3.0 ã®å…¨æŒ‡æ•°ã‚’è¨ˆç®—ï¼ˆç°¡æ˜“ç‰ˆï¼‰"""
    print("ğŸ”¢ NAR-SI3.0 å…¨æŒ‡æ•°è¨ˆç®—é–‹å§‹ï¼ˆç°¡æ˜“ç‰ˆï¼‰...")
    
    results = []
    
    for idx, row in df.iterrows():
        try:
            # åŸºæœ¬æƒ…å ±
            keibajo_code = int(row['keibajo_code'])
            kyori = int(row['kyori'])
            wakuban = int(row['wakuban'])
            tosu = int(row['tosu'])
            
            # ã‚¿ã‚¤ãƒ æƒ…å ±
            soha_time_sec = float(row['soha_time_sec'])
            kohan_3f_sec = float(row['kohan_3f_sec'])
            
            # å‰åŠ3Fæ¨å®šï¼ˆ3ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰
            if 'actual_ten_3f' in row and pd.notna(row['actual_ten_3f']):
                # ãƒ‡ãƒ¼ã‚¿ã«å®Ÿæ¸¬å€¤ãŒã‚ã‚‹å ´åˆã¯ãã‚Œã‚’ä½¿ç”¨
                zenhan_3f = float(row['actual_ten_3f'])
            else:
                # è·é›¢åˆ¥ã®æ¨å®š
                if kyori < 1200:
                    # 1200mæœªæº€: èµ°ç ´ã‚¿ã‚¤ãƒ  - å¾ŒåŠ3Fï¼ˆå®Ÿéš›ã¯2Fãªã©ã«ãªã‚‹ãŒè¨±å®¹ï¼‰
                    zenhan_3f = soha_time_sec - kohan_3f_sec
                elif kyori == 1200:
                    # 1200m: èµ°ç ´ã‚¿ã‚¤ãƒ  - å¾ŒåŠ3F
                    zenhan_3f = soha_time_sec - kohan_3f_sec
                else:
                    # 1201mä»¥ä¸Š: ten_3f_estimator.py ã¨åŒã˜ãƒ­ã‚¸ãƒƒã‚¯ï¼ˆç°¡æ˜“ç‰ˆï¼‰
                    # åŸºæº–ã‚¿ã‚¤ãƒ  + ã‚¹ãƒ”ãƒ¼ãƒ‰æŒ‡æ•°è£œæ­£ã®ç°¡æ˜“å®Ÿè£…
                    # æ³¨: æœ¬å®Ÿè£…ã§ã¯ten_3f_estimator.pyã‚’ä½¿ç”¨ã™ã¹ãã ãŒã€
                    # ãƒ‡ãƒ¼ã‚¿ç¢ºèªç”¨ã‚¹ã‚¯ãƒªãƒ—ãƒˆãªã®ã§ç°¡æ˜“ç‰ˆã‚’ä½¿ç”¨
                    if kyori <= 1400:
                        ratio = 0.26  # å‰åŠ3F â‰ˆ èµ°ç ´ã‚¿ã‚¤ãƒ ã®26%
                    elif kyori <= 1600:
                        ratio = 0.22
                    elif kyori <= 1800:
                        ratio = 0.22
                    elif kyori <= 2000:
                        ratio = 0.17
                    else:
                        ratio = 0.16
                    zenhan_3f = soha_time_sec * ratio
                    # ç‰©ç†çš„åˆ¶ç´„
                    zenhan_3f = max(30.0, min(45.0, zenhan_3f))
            
            # ã‚³ãƒ¼ãƒŠãƒ¼é †ä½
            if 'corner_4' in row and pd.notna(row['corner_4']):
                corner_4 = int(row['corner_4'])
            else:
                corner_4 = int(row['chakujun']) if 'chakujun' in row else tosu // 2
            
            # åŸºæº–ã‚¿ã‚¤ãƒ ï¼ˆç«¶é¦¬å ´ãƒ»è·é›¢åˆ¥ï¼‰
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã¨ã—ã¦39.0ã‚’ä½¿ç”¨ï¼ˆå–å¾—å¤±æ•—æ™‚ï¼‰
            try:
                # ç«¶é¦¬å ´ãƒ»è·é›¢åˆ¥ã®åŸºæº–ã‚¿ã‚¤ãƒ ã‚’å–å¾—
                if kyori <= 1200:
                    base_time = 37.5
                elif kyori <= 1400:
                    base_time = 38.0
                elif kyori <= 1600:
                    base_time = 39.0
                elif kyori <= 1800:
                    base_time = 39.5
                elif kyori <= 2000:
                    base_time = 40.0
                else:  # 2000mè¶…
                    base_time = 40.5
            except:
                base_time = 39.0
            
            # 1. ä¸ŠãŒã‚ŠæŒ‡æ•°ï¼ˆå®Ÿè£…æº–æ‹ : Ã—1ã€è£œæ­£ã¯çœç•¥ï¼‰
            agari_index = (base_time - kohan_3f_sec)
            agari_index = max(-100, min(100, agari_index))
            
            # 2. ä½ç½®æŒ‡æ•°ï¼ˆã‚³ãƒ¼ãƒŠãƒ¼4è§’é †ä½ãƒ™ãƒ¼ã‚¹ï¼‰
            avg_position = corner_4
            base_position = tosu / 2.0
            position_index = ((base_position - avg_position) / tosu) * 100
            position_index = max(0, min(100, position_index))  # ç¯„å›²åˆ¶é™ã®ã¿è¿½åŠ 
            
            # 3. ãƒ†ãƒ³æŒ‡æ•°ï¼ˆå®Ÿè£…æº–æ‹ : Ã—1ã€è£œæ­£ã¯çœç•¥ï¼‰
            ten_index = (base_time - zenhan_3f)
            ten_index = max(-100, min(100, ten_index))
            
            # 4. ãƒšãƒ¼ã‚¹æŒ‡æ•°ï¼ˆå®Ÿè£…æº–æ‹ : å¹³å‡ã€è£œæ­£ã¯çœç•¥ï¼‰
            pace_index = (ten_index + agari_index) / 2
            pace_index = max(-100, min(100, pace_index))
            
            results.append({
                'race_id': row['race_id'],
                'umaban': row['umaban'],
                'chakujun': row['chakujun'],
                'tosu': tosu,
                'ä¸ŠãŒã‚ŠæŒ‡æ•°': agari_index,
                'ä½ç½®æŒ‡æ•°': position_index,
                'ãƒ†ãƒ³æŒ‡æ•°': ten_index,
                'ãƒšãƒ¼ã‚¹æŒ‡æ•°': pace_index
            })
            
        except Exception as e:
            continue
    
    result_df = pd.DataFrame(results)
    print(f"   æŒ‡æ•°è¨ˆç®—å®Œäº†: {len(result_df):,}é ­\n")
    
    return result_df


# ============================
# çµ±è¨ˆæƒ…å ±è¨ˆç®—
# ============================

def calculate_statistics(df: pd.DataFrame, index_name: str) -> dict:
    """æŒ‡æ•°ã®çµ±è¨ˆæƒ…å ±ã‚’è¨ˆç®—"""
    values = df[index_name].dropna()
    
    stats = {
        'æŒ‡æ•°å': index_name,
        'ãƒ‡ãƒ¼ã‚¿æ•°': len(values),
        'æœ€å°å€¤': values.min(),
        'æœ€å¤§å€¤': values.max(),
        'å¹³å‡å€¤': values.mean(),
        'ä¸­å¤®å€¤': values.median(),
        'æ¨™æº–åå·®': values.std(),
        '5%ç‚¹': values.quantile(0.05),
        '25%ç‚¹': values.quantile(0.25),
        '50%ç‚¹': values.quantile(0.50),
        '75%ç‚¹': values.quantile(0.75),
        '95%ç‚¹': values.quantile(0.95)
    }
    
    return stats


def print_statistics(stats: dict):
    """çµ±è¨ˆæƒ…å ±ã‚’è¡¨ç¤º"""
    print(f"\n{'='*80}")
    print(f"ğŸ“Š {stats['æŒ‡æ•°å']} ã®çµ±è¨ˆæƒ…å ±")
    print(f"{'='*80}")
    print(f"ãƒ‡ãƒ¼ã‚¿æ•°    : {stats['ãƒ‡ãƒ¼ã‚¿æ•°']:,}é ­")
    print(f"\nã€åŸºæœ¬çµ±è¨ˆé‡ã€‘")
    print(f"æœ€å°å€¤      : {stats['æœ€å°å€¤']:>10.2f}")
    print(f"æœ€å¤§å€¤      : {stats['æœ€å¤§å€¤']:>10.2f}")
    print(f"å¹³å‡å€¤      : {stats['å¹³å‡å€¤']:>10.2f}")
    print(f"ä¸­å¤®å€¤      : {stats['ä¸­å¤®å€¤']:>10.2f}")
    print(f"æ¨™æº–åå·®    : {stats['æ¨™æº–åå·®']:>10.2f}")
    print(f"\nã€åˆ†ä½ç‚¹ã€‘")
    print(f"5%ç‚¹       : {stats['5%ç‚¹']:>10.2f}  ï¼ˆä¸‹ä½5%ã®é¦¬ã¯ã“ã®å€¤ä»¥ä¸‹ï¼‰")
    print(f"25%ç‚¹      : {stats['25%ç‚¹']:>10.2f}  ï¼ˆä¸‹ä½25%ã®é¦¬ã¯ã“ã®å€¤ä»¥ä¸‹ï¼‰")
    print(f"50%ç‚¹      : {stats['50%ç‚¹']:>10.2f}  ï¼ˆä¸­å¤®å€¤ï¼‰")
    print(f"75%ç‚¹      : {stats['75%ç‚¹']:>10.2f}  ï¼ˆä¸Šä½25%ã®é¦¬ã¯ã“ã®å€¤ä»¥ä¸Šï¼‰")
    print(f"95%ç‚¹      : {stats['95%ç‚¹']:>10.2f}  ï¼ˆä¸Šä½5%ã®é¦¬ã¯ã“ã®å€¤ä»¥ä¸Šï¼‰")
    print(f"\nã€å€¤ã®ç¯„å›²ã€‘")
    print(f"å¹…          : {stats['æœ€å¤§å€¤'] - stats['æœ€å°å€¤']:>10.2f}  ï¼ˆæœ€å¤§å€¤ - æœ€å°å€¤ï¼‰")


# ============================
# ãƒ¡ã‚¤ãƒ³å‡¦ç†
# ============================

def main():
    print("=" * 100)
    print("ğŸ“Š NAR-SI3.0 å„æŒ‡æ•°ã®å€¤ã®ç¯„å›²ç¢ºèªï¼ˆWindowsç”¨ï¼‰")
    print("=" * 100)
    
    # å¯¾è©±å¼è¨­å®š
    data_path = input(f"ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼ˆç©ºç™½ã§ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰: ").strip() or DEFAULT_DATA_PATH
    start_date = input(f"é–‹å§‹æ—¥ï¼ˆYYYYMMDDã€ç©ºç™½ã§{DEFAULT_START_DATE}ï¼‰: ").strip() or DEFAULT_START_DATE
    end_date = input(f"çµ‚äº†æ—¥ï¼ˆYYYYMMDDã€ç©ºç™½ã§{DEFAULT_END_DATE}ï¼‰: ").strip() or DEFAULT_END_DATE
    sample_rate_str = input(f"ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ç‡ï¼ˆ0.0ï½1.0ã€ç©ºç™½ã§{DEFAULT_SAMPLE_RATE}ï¼‰: ").strip()
    sample_rate = float(sample_rate_str) if sample_rate_str else DEFAULT_SAMPLE_RATE
    
    print("\n" + "=" * 100)
    print(f"ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¹: {data_path}")
    print(f"æœŸé–“: {start_date} ï½ {end_date}")
    print(f"ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ç‡: {int(sample_rate*100)}%")
    print("=" * 100 + "\n")
    
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    df = load_and_filter_data(data_path, start_date, end_date, sample_rate)
    
    # æŒ‡æ•°è¨ˆç®—
    df_with_indices = calculate_all_indices_simple(df)
    
    # å„æŒ‡æ•°ã®çµ±è¨ˆæƒ…å ±ã‚’è¨ˆç®—ãƒ»è¡¨ç¤º
    index_names = ['ä¸ŠãŒã‚ŠæŒ‡æ•°', 'ä½ç½®æŒ‡æ•°', 'ãƒ†ãƒ³æŒ‡æ•°', 'ãƒšãƒ¼ã‚¹æŒ‡æ•°']
    all_stats = []
    
    for index_name in index_names:
        stats = calculate_statistics(df_with_indices, index_name)
        all_stats.append(stats)
        print_statistics(stats)
    
    # ã‚µãƒãƒªãƒ¼ãƒ†ãƒ¼ãƒ–ãƒ«
    print("\n" + "=" * 100)
    print("ğŸ“‹ å…¨æŒ‡æ•°ã‚µãƒãƒªãƒ¼")
    print("=" * 100)
    print(f"{'æŒ‡æ•°å':<12} {'æœ€å°å€¤':>10} {'æœ€å¤§å€¤':>10} {'å¹³å‡å€¤':>10} {'ä¸­å¤®å€¤':>10} {'æ¨™æº–åå·®':>10}")
    print("-" * 100)
    
    for stats in all_stats:
        print(f"{stats['æŒ‡æ•°å']:<12} "
              f"{stats['æœ€å°å€¤']:>10.2f} "
              f"{stats['æœ€å¤§å€¤']:>10.2f} "
              f"{stats['å¹³å‡å€¤']:>10.2f} "
              f"{stats['ä¸­å¤®å€¤']:>10.2f} "
              f"{stats['æ¨™æº–åå·®']:>10.2f}")
    
    # CSV/JSONå‡ºåŠ›
    output_dir = os.path.dirname(data_path) if os.path.dirname(data_path) else '.'
    
    summary_df = pd.DataFrame(all_stats)
    csv_path = os.path.join(output_dir, 'index_range_summary.csv')
    json_path = os.path.join(output_dir, 'index_range_summary.json')
    
    summary_df.to_csv(csv_path, index=False, encoding='utf-8-sig')
    summary_df.to_json(json_path, orient='records', force_ascii=False, indent=2)
    
    print("\n" + "=" * 100)
    print("âœ… çµæœä¿å­˜å®Œäº†")
    print("=" * 100)
    print(f"CSV: {csv_path}")
    print(f"JSON: {json_path}")
    print("=" * 100)
    
    input("\nâœ… å„æŒ‡æ•°ã®å€¤ã®ç¯„å›²ç¢ºèªå®Œäº†ï¼Enterã§çµ‚äº†...")


if __name__ == '__main__':
    main()
