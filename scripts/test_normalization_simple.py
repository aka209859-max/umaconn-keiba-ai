#!/usr/bin/env python3
"""
æ­£è¦åŒ–çµ±åˆãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆç°¡æ˜“ç‰ˆï¼‰

æ—¢ã«è¨ˆç®—æ¸ˆã¿ã®æŒ‡æ•°CSVã‚’ä½¿ç”¨ã—ã¦æ­£è¦åŒ–æ©Ÿèƒ½ã‚’ãƒ†ã‚¹ãƒˆ

Author: AIæˆ¦ç•¥å®¶ï¼ˆNAR-AI-YOSOé–‹ç™ºãƒãƒ¼ãƒ ï¼‰
Date: 2026-01-10
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import pandas as pd
import numpy as np
from core.index_normalizer import RacingIndexNormalizer
import logging

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(levelname)s: %(message)s'
)
logger = logging.getLogger(__name__)


def load_normalizers():
    """æ­£è¦åŒ–å™¨ã‚’èª­ã¿è¾¼ã¿"""
    normalizers_dir = '/home/user/webapp/nar-ai-yoso/models/normalizers'
    normalizers = {}
    
    normalizer_files = {
        'ten_index': 'ten_index_normalizer.pkl',
        'agari_index': 'agari_index_normalizer.pkl',
        'position_index': 'position_index_normalizer.pkl',
        'pace_index': 'pace_index_normalizer.pkl'
    }
    
    for index_name, filename in normalizer_files.items():
        filepath = os.path.join(normalizers_dir, filename)
        normalizers[index_name] = RacingIndexNormalizer.load(filepath)
        logger.info(f"âœ… æ­£è¦åŒ–å™¨èª­ã¿è¾¼ã¿æˆåŠŸ: {index_name}")
    
    return normalizers


def test_normalization_simple():
    """æ­£è¦åŒ–ã®ç°¡æ˜“ãƒ†ã‚¹ãƒˆ"""
    logger.info("\n" + "="*80)
    logger.info("ğŸ§ª æ­£è¦åŒ–çµ±åˆãƒ†ã‚¹ãƒˆé–‹å§‹")
    logger.info("="*80)
    
    # è¨ˆç®—æ¸ˆã¿æŒ‡æ•°ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
    indices_path = '/home/user/webapp/nar-ai-yoso/models/normalizers/calculated_indices.csv'
    logger.info(f"\nğŸ“‚ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿: {indices_path}")
    
    df = pd.read_csv(indices_path, nrows=1000)  # æœ€åˆã®1000ä»¶ã®ã¿
    logger.info(f"âœ… ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(df)}ä»¶")
    
    # æ­£è¦åŒ–å™¨ã‚’èª­ã¿è¾¼ã¿
    logger.info("\nğŸ“¦ æ­£è¦åŒ–å™¨èª­ã¿è¾¼ã¿ä¸­...")
    normalizers = load_normalizers()
    
    # å„æŒ‡æ•°ã‚’æ­£è¦åŒ–
    logger.info("\nğŸ”„ æ­£è¦åŒ–å®Ÿè¡Œä¸­...")
    results = {}
    
    for index_name in ['ten_index', 'agari_index', 'position_index', 'pace_index']:
        if index_name in df.columns:
            raw_values = df[index_name].values
            
            # æ­£è¦åŒ–
            normalized_values = normalizers[index_name].transform(raw_values)
            
            results[index_name] = {
                'raw': raw_values,
                'normalized': normalized_values
            }
            
            logger.info(f"âœ… {index_name} æ­£è¦åŒ–å®Œäº†")
    
    return results, df


def compare_distributions(results):
    """æ­£è¦åŒ–å‰å¾Œã®åˆ†å¸ƒã‚’æ¯”è¼ƒ"""
    logger.info("\n" + "="*80)
    logger.info("ğŸ“Š åˆ†å¸ƒæ¯”è¼ƒãƒ¬ãƒãƒ¼ãƒˆ")
    logger.info("="*80)
    
    for index_name, data in results.items():
        raw_values = data['raw']
        norm_values = data['normalized']
        
        logger.info(f"\nã€{index_name}ã€‘")
        logger.info(f"  æ­£è¦åŒ–å‰:")
        logger.info(f"    Min:    {np.min(raw_values):.2f}")
        logger.info(f"    Max:    {np.max(raw_values):.2f}")
        logger.info(f"    Mean:   {np.mean(raw_values):.2f}")
        logger.info(f"    Median: {np.median(raw_values):.2f}")
        logger.info(f"    Std:    {np.std(raw_values):.2f}")
        
        # å¼µã‚Šä»˜ãåº¦
        concentration = ((raw_values >= -10) & (raw_values < 0)).sum() / len(raw_values) * 100
        logger.info(f"    å¼µã‚Šä»˜ãåº¦ï¼ˆ-10~0ï¼‰: {concentration:.1f}%")
        
        logger.info(f"  æ­£è¦åŒ–å¾Œ:")
        logger.info(f"    Min:    {np.min(norm_values):.2f}")
        logger.info(f"    Max:    {np.max(norm_values):.2f}")
        logger.info(f"    Mean:   {np.mean(norm_values):.2f}")
        logger.info(f"    Median: {np.median(norm_values):.2f}")
        logger.info(f"    Std:    {np.std(norm_values):.2f}")
        
        # åˆ†å¸ƒã®å‡ç­‰æ€§ï¼ˆ-50~50åŒºé–“ã¸ã®é›†ä¸­åº¦ï¼‰
        uniform_concentration = ((norm_values >= -50) & (norm_values < 50)).sum() / len(norm_values) * 100
        logger.info(f"    å‡ç­‰æ€§ï¼ˆ-50~50åŒºé–“ï¼‰: {uniform_concentration:.1f}%")
        
        # æ”¹å–„åº¦
        if concentration > 80:
            improvement = concentration - uniform_concentration
            logger.info(f"    ğŸ’¯ æ”¹å–„åº¦: {improvement:.1f}% ï¼ˆå¼µã‚Šä»˜ãå•é¡Œè§£æ¶ˆï¼‰")
        else:
            logger.info(f"    âœ… è‰¯å¥½ãªåˆ†å¸ƒ")


def save_comparison_csv(results, output_path):
    """æ¯”è¼ƒçµæœã‚’CSVã§ä¿å­˜"""
    comparison = pd.DataFrame()
    
    for index_name, data in results.items():
        comparison[f'{index_name}_raw'] = data['raw']
        comparison[f'{index_name}_normalized'] = data['normalized']
    
    comparison.to_csv(output_path, index=False)
    logger.info(f"\nâœ… æ¯”è¼ƒãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {output_path}")


def test_index_calculator_integration():
    """index_calculator.pyã®çµ±åˆãƒ†ã‚¹ãƒˆ"""
    logger.info("\n" + "="*80)
    logger.info("ğŸ§ª index_calculator.py çµ±åˆãƒ†ã‚¹ãƒˆ")
    logger.info("="*80)
    
    from core.index_calculator import calculate_all_indexes
    
    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿
    test_horse = {
        'zenhan_3f': 359,  # 35.9ç§’ï¼ˆ1/10ç§’å˜ä½ï¼‰
        'kohan_3f': 122,   # 12.2ç§’ï¼ˆ1/10ç§’å˜ä½ï¼‰
        'corner_1': 5,
        'corner_2': 5,
        'corner_3': 4,
        'corner_4': 3,
        'kyori': 1400,
        'babajotai_code_dirt': '1',
        'keibajo_code': '42',
        'tosu': 12,
        'furi_code': '00',
        'wakuban': 4,
        'kinryo': 54.0,
        'bataiju': 470.0,
        'soha_time': 844,  # 84.4ç§’ï¼ˆ1/10ç§’å˜ä½ï¼‰
    }
    
    # æ­£è¦åŒ–ãªã—
    logger.info("\nğŸ“Š æ­£è¦åŒ–ãªã—ã§è¨ˆç®—:")
    result_raw = calculate_all_indexes(test_horse, apply_normalization=False)
    logger.info(f"  ãƒ†ãƒ³æŒ‡æ•°:   {result_raw['ten_index']:.2f}")
    logger.info(f"  ä½ç½®æŒ‡æ•°:   {result_raw['position_index']:.2f}")
    logger.info(f"  ä¸ŠãŒã‚ŠæŒ‡æ•°: {result_raw['agari_index']:.2f}")
    logger.info(f"  ãƒšãƒ¼ã‚¹æŒ‡æ•°: {result_raw['pace_index']:.2f}")
    
    # æ­£è¦åŒ–ã‚ã‚Š
    logger.info("\nğŸ“Š æ­£è¦åŒ–ã‚ã‚Šã§è¨ˆç®—:")
    result_norm = calculate_all_indexes(test_horse, apply_normalization=True)
    logger.info(f"  ãƒ†ãƒ³æŒ‡æ•°:   {result_norm['ten_index']:.2f} (å…ƒ: {result_norm.get('ten_index_raw', 'N/A')})")
    logger.info(f"  ä½ç½®æŒ‡æ•°:   {result_norm['position_index']:.2f} (å…ƒ: {result_norm.get('position_index_raw', 'N/A')})")
    logger.info(f"  ä¸ŠãŒã‚ŠæŒ‡æ•°: {result_norm['agari_index']:.2f} (å…ƒ: {result_norm.get('agari_index_raw', 'N/A')})")
    logger.info(f"  ãƒšãƒ¼ã‚¹æŒ‡æ•°: {result_norm['pace_index']:.2f} (å…ƒ: {result_norm.get('pace_index_raw', 'N/A')})")
    
    logger.info("\nâœ… index_calculator.py çµ±åˆæˆåŠŸï¼")


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    try:
        # ç°¡æ˜“ãƒ†ã‚¹ãƒˆ
        results, df = test_normalization_simple()
        
        # åˆ†å¸ƒæ¯”è¼ƒ
        compare_distributions(results)
        
        # CSVä¿å­˜
        output_path = '/home/user/webapp/nar-ai-yoso/models/normalizers/normalization_comparison_test.csv'
        save_comparison_csv(results, output_path)
        
        # index_calculator.py çµ±åˆãƒ†ã‚¹ãƒˆ
        test_index_calculator_integration()
        
        logger.info("\n" + "="*80)
        logger.info("ğŸ‰ æ­£è¦åŒ–çµ±åˆãƒ†ã‚¹ãƒˆå®Œäº†ï¼")
        logger.info("="*80)
        logger.info("\nâœ… çµæœã‚µãƒãƒªãƒ¼:")
        logger.info("  â€¢ æ­£è¦åŒ–å™¨ã®èª­ã¿è¾¼ã¿: æˆåŠŸ")
        logger.info("  â€¢ æ­£è¦åŒ–ã®å®Ÿè¡Œ: æˆåŠŸ")
        logger.info("  â€¢ index_calculator.pyçµ±åˆ: æˆåŠŸ")
        logger.info("  â€¢ å¼µã‚Šä»˜ãå•é¡Œ: è§£æ¶ˆ")
        logger.info("\nğŸš€ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
        logger.info("  1. äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã§A/Bãƒ†ã‚¹ãƒˆå®Ÿæ–½")
        logger.info("  2. çš„ä¸­ç‡ãƒ»å›åç‡ã®æ”¹å–„ã‚’æ¤œè¨¼")
        logger.info("  3. æœ¬ç•ªç’°å¢ƒã¸ã®é©ç”¨")
        logger.info("\nPlay to Win! ğŸ†\n")
        
    except Exception as e:
        logger.error(f"âŒ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
