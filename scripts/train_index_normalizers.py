#!/usr/bin/env python3
"""
æŒ‡æ•°æ­£è¦åŒ–å™¨ã®å­¦ç¿’ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

éå»ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰4æŒ‡æ•°ï¼ˆãƒ†ãƒ³ãƒ»ä¸ŠãŒã‚Šãƒ»ä½ç½®ãƒ»ãƒšãƒ¼ã‚¹ï¼‰ã®æ­£è¦åŒ–å™¨ã‚’å­¦ç¿’ã—ã€
å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã™ã‚‹ã€‚

ä½¿ç”¨æ–¹æ³•:
    python scripts/train_index_normalizers.py

å‡ºåŠ›:
    models/normalizers/
    â”œâ”€â”€ ten_index_normalizer.pkl          # ãƒ†ãƒ³æŒ‡æ•°ç”¨
    â”œâ”€â”€ agari_index_normalizer.pkl        # ä¸ŠãŒã‚ŠæŒ‡æ•°ç”¨
    â”œâ”€â”€ position_index_normalizer.pkl     # ä½ç½®æŒ‡æ•°ç”¨
    â””â”€â”€ pace_index_normalizer.pkl         # ãƒšãƒ¼ã‚¹æŒ‡æ•°ç”¨

Author: AIæˆ¦ç•¥å®¶ï¼ˆNAR-AI-YOSOé–‹ç™ºãƒãƒ¼ãƒ ï¼‰
Date: 2026-01-10
"""

import sys
import os

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import pandas as pd
import numpy as np
import logging
from core.index_normalizer import RacingIndexNormalizer
from core.index_calculator import calculate_ten_index, calculate_agari_index, calculate_position_index, calculate_pace_index
from config.base_times import get_base_time

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ============================
# è¨­å®š
# ============================

# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š
DEFAULT_DATA_PATH = r'E:\UmaData\nar-analytics-python-v2\data-1768047611955.csv'
DEFAULT_START_DATE = '20231013'  # å¤§äº•ç ‚å…¥ã‚Œæ›¿ãˆå¾Œ
DEFAULT_END_DATE = '20251231'
DEFAULT_SAMPLE_RATE = 0.1  # 10%ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆãƒ†ã‚¹ãƒˆç”¨ï¼‰
DEFAULT_OUTPUT_DIR = 'models/normalizers'

# ============================
# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
# ============================

def load_and_filter_data(
    file_path: str,
    start_date: str,
    end_date: str,
    sample_rate: float = 0.1
) -> pd.DataFrame:
    """
    ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    
    Args:
        file_path: ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        start_date: é–‹å§‹æ—¥ï¼ˆYYYYMMDDï¼‰
        end_date: çµ‚äº†æ—¥ï¼ˆYYYYMMDDï¼‰
        sample_rate: ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ç‡ï¼ˆ0.0ï½1.0ï¼‰
    
    Returns:
        ãƒ•ã‚£ãƒ«ã‚¿æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
    """
    logger.info(f"ğŸ“ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿: {file_path}")
    
    # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°èª­ã¿è¾¼ã¿
    df = pd.read_csv(file_path, skiprows=lambda i: i > 0 and np.random.rand() > sample_rate)
    logger.info(f"   èª­ã¿è¾¼ã¿: {len(df):,}è¡Œï¼ˆã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°{int(sample_rate*100)}%ï¼‰")
    
    # æœŸé–“ãƒ•ã‚£ãƒ«ã‚¿
    df['race_date'] = df['race_date'].astype(str)
    df = df[(df['race_date'] >= start_date) & (df['race_date'] <= end_date)]
    logger.info(f"   æœŸé–“ãƒ•ã‚£ãƒ«ã‚¿: {len(df):,}è¡Œï¼ˆ{start_date}ï½{end_date}ï¼‰")
    
    # ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°
    df = df.dropna(subset=['race_date', 'keibajo_code', 'kyori', 'wakuban', 'chakujun',
                            'soha_time_sec', 'kohan_3f_sec', 'weight_kg', 'tosu'])
    df = df[df['soha_time_sec'] > 0]
    df = df[df['kohan_3f_sec'] > 0]
    df = df[df['tosu'] >= 4]
    logger.info(f"   ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°å¾Œ: {len(df):,}è¡Œ\n")
    
    return df


# ============================
# æŒ‡æ•°è¨ˆç®—
# ============================

def calculate_all_indices(df: pd.DataFrame) -> pd.DataFrame:
    """
    å…¨æŒ‡æ•°ã‚’è¨ˆç®—ï¼ˆå®Ÿè£…ç‰ˆã®ãƒ­ã‚¸ãƒƒã‚¯ã‚’ä½¿ç”¨ï¼‰
    
    Args:
        df: å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
    
    Returns:
        æŒ‡æ•°ã‚’å«ã‚€ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
    """
    logger.info("ğŸ”¢ NAR-SI3.0 å…¨æŒ‡æ•°è¨ˆç®—é–‹å§‹ï¼ˆå®Ÿè£…ç‰ˆãƒ­ã‚¸ãƒƒã‚¯ä½¿ç”¨ï¼‰...")
    
    results = []
    
    for idx, row in df.iterrows():
        try:
            # åŸºæœ¬æƒ…å ±
            keibajo_code = str(int(row['keibajo_code']))
            kyori = int(row['kyori'])
            wakuban = int(row['wakuban']) if pd.notna(row['wakuban']) else 0
            tosu = int(row['tosu'])
            
            # ã‚¿ã‚¤ãƒ æƒ…å ±
            soha_time_sec = float(row['soha_time_sec'])
            kohan_3f_sec = float(row['kohan_3f_sec'])
            
            # å‰åŠ3Fæ¨å®šï¼ˆ3ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰
            if 'actual_ten_3f' in row and pd.notna(row['actual_ten_3f']):
                zenhan_3f = float(row['actual_ten_3f'])
            else:
                if kyori <= 1200:
                    # 1200mä»¥ä¸‹: èµ°ç ´ã‚¿ã‚¤ãƒ  - å¾ŒåŠ3F
                    zenhan_3f = soha_time_sec - kohan_3f_sec
                else:
                    # 1201mä»¥ä¸Š: è·é›¢åˆ¥æ¯”ç‡ï¼ˆç°¡æ˜“ç‰ˆï¼‰
                    if kyori <= 1400:
                        ratio = 0.26
                    elif kyori <= 1600:
                        ratio = 0.22
                    elif kyori <= 1800:
                        ratio = 0.22
                    elif kyori <= 2000:
                        ratio = 0.17
                    else:
                        ratio = 0.16
                    zenhan_3f = soha_time_sec * ratio
                    zenhan_3f = max(30.0, min(45.0, zenhan_3f))
            
            # ã‚³ãƒ¼ãƒŠãƒ¼é †ä½
            if 'corner_4' in row and pd.notna(row['corner_4']):
                corner_4 = int(row['corner_4'])
            else:
                corner_4 = int(row['chakujun']) if 'chakujun' in row else tosu // 2
            
            # åŸºæº–ã‚¿ã‚¤ãƒ å–å¾—
            try:
                base_time_kohan = get_base_time(keibajo_code, kyori, 'kohan_3f')
                base_time_zenhan = get_base_time(keibajo_code, kyori, 'zenhan_3f')
            except:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: è·é›¢åˆ¥ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
                if kyori <= 1200:
                    base_time_kohan = 37.5
                    base_time_zenhan = 37.5
                elif kyori <= 1400:
                    base_time_kohan = 38.0
                    base_time_zenhan = 38.0
                elif kyori <= 1600:
                    base_time_kohan = 39.0
                    base_time_zenhan = 39.0
                elif kyori <= 1800:
                    base_time_kohan = 39.5
                    base_time_zenhan = 39.5
                elif kyori <= 2000:
                    base_time_kohan = 40.0
                    base_time_zenhan = 40.0
                else:
                    base_time_kohan = 40.5
                    base_time_zenhan = 40.5
            
            # 1. ä¸ŠãŒã‚ŠæŒ‡æ•°ï¼ˆå®Ÿè£…æº–æ‹ : Ã—1ã€è£œæ­£ã¯çœç•¥ï¼‰
            agari_index = base_time_kohan - kohan_3f_sec
            
            # 2. ä½ç½®æŒ‡æ•°ï¼ˆå®Ÿè£…æº–æ‹ ï¼‰
            avg_position = corner_4
            base_position = tosu / 2.0
            position_index = 100 - (avg_position / tosu) * 100
            
            # 3. ãƒ†ãƒ³æŒ‡æ•°ï¼ˆå®Ÿè£…æº–æ‹ : Ã—1ã€è£œæ­£ã¯çœç•¥ï¼‰
            ten_index = base_time_zenhan - zenhan_3f
            
            # 4. ãƒšãƒ¼ã‚¹æŒ‡æ•°ï¼ˆå®Ÿè£…æº–æ‹ : å¹³å‡ã€è£œæ­£ã¯çœç•¥ï¼‰
            pace_index = (ten_index + agari_index) / 2
            
            # ç¯„å›²åˆ¶é™ãªã—ï¼ˆç”Ÿãƒ‡ãƒ¼ã‚¿ã®ã¾ã¾ï¼‰
            results.append({
                'race_id': row['race_id'],
                'umaban': row['umaban'],
                'chakujun': row['chakujun'],
                'tosu': tosu,
                'ten_index_raw': ten_index,
                'agari_index_raw': agari_index,
                'position_index_raw': position_index,
                'pace_index_raw': pace_index
            })
            
        except Exception as e:
            logger.debug(f"è¡Œ {idx} ã§ã‚¨ãƒ©ãƒ¼: {e}")
            continue
    
    result_df = pd.DataFrame(results)
    logger.info(f"   æŒ‡æ•°è¨ˆç®—å®Œäº†: {len(result_df):,}é ­\n")
    
    return result_df


# ============================
# æ­£è¦åŒ–å™¨ã®å­¦ç¿’
# ============================

def train_normalizers(
    df: pd.DataFrame,
    output_dir: str = DEFAULT_OUTPUT_DIR
) -> dict:
    """
    4æŒ‡æ•°ã®æ­£è¦åŒ–å™¨ã‚’å­¦ç¿’ãƒ»ä¿å­˜
    
    Args:
        df: æŒ‡æ•°ãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚€ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
        output_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
    
    Returns:
        å­¦ç¿’æ¸ˆã¿æ­£è¦åŒ–å™¨ã®è¾æ›¸
    """
    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    os.makedirs(output_dir, exist_ok=True)
    
    normalizers = {}
    
    # 1. ãƒ†ãƒ³æŒ‡æ•°
    logger.info("=" * 80)
    logger.info("ğŸ“Š ãƒ†ãƒ³æŒ‡æ•°ã®æ­£è¦åŒ–å™¨ã‚’å­¦ç¿’ä¸­...")
    logger.info("=" * 80)
    ten_data = df['ten_index_raw'].dropna().values
    ten_normalizer = RacingIndexNormalizer()
    ten_normalizer.fit(ten_data)
    ten_normalizer.save(os.path.join(output_dir, 'ten_index_normalizer.pkl'))
    normalizers['ten'] = ten_normalizer
    logger.info("")
    
    # 2. ä¸ŠãŒã‚ŠæŒ‡æ•°
    logger.info("=" * 80)
    logger.info("ğŸ“Š ä¸ŠãŒã‚ŠæŒ‡æ•°ã®æ­£è¦åŒ–å™¨ã‚’å­¦ç¿’ä¸­...")
    logger.info("=" * 80)
    agari_data = df['agari_index_raw'].dropna().values
    agari_normalizer = RacingIndexNormalizer()
    agari_normalizer.fit(agari_data)
    agari_normalizer.save(os.path.join(output_dir, 'agari_index_normalizer.pkl'))
    normalizers['agari'] = agari_normalizer
    logger.info("")
    
    # 3. ä½ç½®æŒ‡æ•°
    logger.info("=" * 80)
    logger.info("ğŸ“Š ä½ç½®æŒ‡æ•°ã®æ­£è¦åŒ–å™¨ã‚’å­¦ç¿’ä¸­...")
    logger.info("=" * 80)
    position_data = df['position_index_raw'].dropna().values
    position_normalizer = RacingIndexNormalizer(target_range=(0, 100))  # ä½ç½®æŒ‡æ•°ã¯ 0ï½100
    position_normalizer.fit(position_data)
    position_normalizer.save(os.path.join(output_dir, 'position_index_normalizer.pkl'))
    normalizers['position'] = position_normalizer
    logger.info("")
    
    # 4. ãƒšãƒ¼ã‚¹æŒ‡æ•°
    logger.info("=" * 80)
    logger.info("ğŸ“Š ãƒšãƒ¼ã‚¹æŒ‡æ•°ã®æ­£è¦åŒ–å™¨ã‚’å­¦ç¿’ä¸­...")
    logger.info("=" * 80)
    pace_data = df['pace_index_raw'].dropna().values
    pace_normalizer = RacingIndexNormalizer()
    pace_normalizer.fit(pace_data)
    pace_normalizer.save(os.path.join(output_dir, 'pace_index_normalizer.pkl'))
    normalizers['pace'] = pace_normalizer
    logger.info("")
    
    logger.info("=" * 80)
    logger.info(f"âœ… å…¨ã¦ã®æ­£è¦åŒ–å™¨ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {output_dir}")
    logger.info("=" * 80)
    
    return normalizers


# ============================
# æ­£è¦åŒ–å™¨ã®ãƒ†ã‚¹ãƒˆ
# ============================

def test_normalizers(df: pd.DataFrame, normalizers: dict):
    """
    æ­£è¦åŒ–å™¨ã®ãƒ†ã‚¹ãƒˆ
    
    Args:
        df: ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿
        normalizers: æ­£è¦åŒ–å™¨ã®è¾æ›¸
    """
    logger.info("\n" + "=" * 80)
    logger.info("ğŸ§ª æ­£è¦åŒ–å™¨ã®ãƒ†ã‚¹ãƒˆ")
    logger.info("=" * 80)
    
    # ãƒ†ã‚¹ãƒˆç”¨ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºï¼ˆå…ˆé ­1000ä»¶ï¼‰
    test_df = df.head(1000).copy()
    
    # å„æŒ‡æ•°ã‚’æ­£è¦åŒ–
    test_df['ten_index_normalized'] = normalizers['ten'].transform(test_df['ten_index_raw'].values)
    test_df['agari_index_normalized'] = normalizers['agari'].transform(test_df['agari_index_raw'].values)
    test_df['position_index_normalized'] = normalizers['position'].transform(test_df['position_index_raw'].values)
    test_df['pace_index_normalized'] = normalizers['pace'].transform(test_df['pace_index_raw'].values)
    
    # çµ±è¨ˆæƒ…å ±ã®è¡¨ç¤º
    for index_name in ['ten', 'agari', 'position', 'pace']:
        raw_col = f'{index_name}_index_raw'
        norm_col = f'{index_name}_index_normalized'
        
        logger.info(f"\nğŸ“Š {index_name.upper()} Index:")
        logger.info(f"  ç”Ÿãƒ‡ãƒ¼ã‚¿:")
        logger.info(f"    æœ€å°å€¤: {test_df[raw_col].min():.2f}")
        logger.info(f"    æœ€å¤§å€¤: {test_df[raw_col].max():.2f}")
        logger.info(f"    å¹³å‡å€¤: {test_df[raw_col].mean():.2f}")
        logger.info(f"    æ¨™æº–åå·®: {test_df[raw_col].std():.2f}")
        
        logger.info(f"  æ­£è¦åŒ–å¾Œ:")
        logger.info(f"    æœ€å°å€¤: {test_df[norm_col].min():.2f}")
        logger.info(f"    æœ€å¤§å€¤: {test_df[norm_col].max():.2f}")
        logger.info(f"    å¹³å‡å€¤: {test_df[norm_col].mean():.2f}")
        logger.info(f"    æ¨™æº–åå·®: {test_df[norm_col].std():.2f}")
        
        # -100/-95 ã®é›†ä¸­åº¦ãƒã‚§ãƒƒã‚¯
        if index_name in ['ten', 'pace']:
            target_range = (-100, -95)
        elif index_name == 'position':
            target_range = (0, 5)
        else:
            target_range = None
        
        if target_range:
            count_raw = ((test_df[raw_col] >= target_range[0]) & 
                        (test_df[raw_col] <= target_range[1])).sum()
            count_norm = ((test_df[norm_col] >= target_range[0]) & 
                         (test_df[norm_col] <= target_range[1])).sum()
            
            logger.info(f"  {target_range} ç¯„å›²ã®ä»¶æ•°:")
            logger.info(f"    ç”Ÿãƒ‡ãƒ¼ã‚¿: {count_raw}ä»¶ ({count_raw/len(test_df)*100:.1f}%)")
            logger.info(f"    æ­£è¦åŒ–å¾Œ: {count_norm}ä»¶ ({count_norm/len(test_df)*100:.1f}%)")
    
    logger.info("\n" + "=" * 80)


# ============================
# ãƒ¡ã‚¤ãƒ³å‡¦ç†
# ============================

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("=" * 100)
    print("ğŸš€ æŒ‡æ•°æ­£è¦åŒ–å™¨ã®å­¦ç¿’ã‚¹ã‚¯ãƒªãƒ—ãƒˆ")
    print("=" * 100)
    
    # å¯¾è©±å¼è¨­å®š
    data_path = input(f"ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼ˆç©ºç™½ã§ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰: ").strip() or DEFAULT_DATA_PATH
    start_date = input(f"é–‹å§‹æ—¥ï¼ˆYYYYMMDDã€ç©ºç™½ã§{DEFAULT_START_DATE}ï¼‰: ").strip() or DEFAULT_START_DATE
    end_date = input(f"çµ‚äº†æ—¥ï¼ˆYYYYMMDDã€ç©ºç™½ã§{DEFAULT_END_DATE}ï¼‰: ").strip() or DEFAULT_END_DATE
    sample_rate_str = input(f"ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ç‡ï¼ˆ0.0ï½1.0ã€ç©ºç™½ã§{DEFAULT_SAMPLE_RATE}ï¼‰: ").strip()
    sample_rate = float(sample_rate_str) if sample_rate_str else DEFAULT_SAMPLE_RATE
    output_dir = input(f"å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆç©ºç™½ã§{DEFAULT_OUTPUT_DIR}ï¼‰: ").strip() or DEFAULT_OUTPUT_DIR
    
    print("\n" + "=" * 100)
    print(f"ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¹: {data_path}")
    print(f"æœŸé–“: {start_date} ï½ {end_date}")
    print(f"ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ç‡: {int(sample_rate*100)}%")
    print(f"å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {output_dir}")
    print("=" * 100 + "\n")
    
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    df = load_and_filter_data(data_path, start_date, end_date, sample_rate)
    
    # æŒ‡æ•°è¨ˆç®—
    df_with_indices = calculate_all_indices(df)
    
    # æ­£è¦åŒ–å™¨ã®å­¦ç¿’
    normalizers = train_normalizers(df_with_indices, output_dir)
    
    # ãƒ†ã‚¹ãƒˆ
    test_normalizers(df_with_indices, normalizers)
    
    print("\n" + "=" * 100)
    print("âœ… å…¨ã¦ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
    print("=" * 100)


if __name__ == '__main__':
    main()
