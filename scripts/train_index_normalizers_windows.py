"""
æŒ‡æ•°æ­£è¦åŒ–å™¨ã®å­¦ç¿’ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆWindowsç‰ˆãƒ»Standaloneï¼‰

ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ core/ ã¸ã®ä¾å­˜ãªã—ã§å˜ç‹¬ã§å‹•ä½œã—ã¾ã™ã€‚
å…¨ã¦ã®å¿…è¦ãªã‚³ãƒ¼ãƒ‰ãŒçµ±åˆã•ã‚Œã¦ã„ã¾ã™ã€‚

ç›®çš„:
- NAR-SI3.0ã®4æŒ‡æ•°ï¼ˆãƒ†ãƒ³æŒ‡æ•°ã€ä¸ŠãŒã‚ŠæŒ‡æ•°ã€ä½ç½®æŒ‡æ•°ã€ãƒšãƒ¼ã‚¹æŒ‡æ•°ï¼‰ã‚’æ­£è¦åŒ–
- RankGaussï¼ˆQuantile Transformationï¼‰ã«ã‚ˆã‚‹çµ±è¨ˆçš„æ­£è¦åŒ–
- 70%ã®å¼µã‚Šä»˜ãå•é¡Œã‚’è§£æ¶ˆ

ä½¿ç”¨æ–¹æ³•:
    cd /d E:\\UmaData\\nar-analytics-python-v2
    python train_index_normalizers_windows.py

å‡ºåŠ›:
    models/normalizers/ten_index_normalizer.pkl
    models/normalizers/agari_index_normalizer.pkl
    models/normalizers/position_index_normalizer.pkl
    models/normalizers/pace_index_normalizer.pkl

Author: AIæˆ¦ç•¥å®¶ï¼ˆNAR-AI-YOSOé–‹ç™ºãƒãƒ¼ãƒ ï¼‰
Date: 2026-01-10
"""

import os
import sys
import pandas as pd
import numpy as np
import joblib
import logging
from typing import Optional, Tuple, Dict
from sklearn.preprocessing import QuantileTransformer
from datetime import datetime

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


# ============================
# RacingIndexNormalizer ã‚¯ãƒ©ã‚¹ï¼ˆåŸ‹ã‚è¾¼ã¿ç‰ˆï¼‰
# ============================

class RacingIndexNormalizer:
    """
    ç«¶é¦¬æŒ‡æ•°ã®çµ±è¨ˆçš„æ­£è¦åŒ–ã‚¯ãƒ©ã‚¹
    
    RankGauss (Quantile Transformation) ã«ã‚ˆã‚‹æ­£è¦åŒ–:
    1. ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ©ãƒ³ã‚¯ï¼ˆé †ä½ï¼‰ã«å¤‰æ›
    2. æ­£è¦åˆ†å¸ƒã®ç´¯ç©åˆ†å¸ƒé–¢æ•°ï¼ˆCDFï¼‰ã«åŸºã¥ã„ã¦å€¤ã‚’å†é…ç½®
    3. 4ÏƒåŸºæº–ã§ [-100, 100] ã®ç¯„å›²ã«ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
    """
    
    def __init__(
        self, 
        target_range: Tuple[float, float] = (-100, 100), 
        sigma_cap: float = 4.0,
        n_quantiles: int = 2000,
        random_state: int = 42
    ):
        """
        åˆæœŸåŒ–
        
        Args:
            target_range: ç›®æ¨™ç¯„å›²ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: (-100, 100)ï¼‰
            sigma_cap: ÏƒåŸºæº–ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 4.0 = 99.99%ã®ãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚€ï¼‰
            n_quantiles: åˆ†ä½ç‚¹ã®æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 2000ã€è©³ç´°ãªåˆ†è§£èƒ½ï¼‰
            random_state: ä¹±æ•°ã‚·ãƒ¼ãƒ‰ï¼ˆå†ç¾æ€§ã®ãŸã‚ï¼‰
        """
        self.target_range = target_range
        self.sigma_cap = sigma_cap
        self.n_quantiles = n_quantiles
        self.random_state = random_state
        
        # QuantileTransformer ã®åˆæœŸåŒ–
        self.qt = QuantileTransformer(
            n_quantiles=n_quantiles,
            output_distribution='normal',
            random_state=random_state,
            subsample=1000000
        )
        
        # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ä¿‚æ•°ï¼ˆ4Ïƒ = 100ç‚¹ï¼‰
        self.scale_factor = target_range[1] / sigma_cap
        
        # å­¦ç¿’æ¸ˆã¿ãƒ•ãƒ©ã‚°
        self.is_fitted = False
        
        logger.info(f"RacingIndexNormalizer initialized: "
                   f"target_range={target_range}, sigma_cap={sigma_cap}, "
                   f"n_quantiles={n_quantiles}")
    
    def fit(self, X: np.ndarray) -> 'RacingIndexNormalizer':
        """éå»ãƒ‡ãƒ¼ã‚¿ã‚’ç”¨ã„ã¦åˆ†å¸ƒã‚’å­¦ç¿’"""
        X = self._validate_input(X)
        
        if len(X) == 0:
            raise ValueError("å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™")
        
        X_2d = X.reshape(-1, 1)
        
        logger.info(f"å­¦ç¿’é–‹å§‹: {len(X):,}ä»¶ã®ãƒ‡ãƒ¼ã‚¿")
        self.qt.fit(X_2d)
        self.is_fitted = True
        logger.info("å­¦ç¿’å®Œäº†")
        
        self._log_statistics(X)
        
        return self
    
    def transform(self, X: np.ndarray) -> np.ndarray:
        """æŒ‡æ•°ã‚’æ­£è¦åŒ–"""
        if not self.is_fitted:
            raise RuntimeError("fit() ã‚’å…ˆã«å®Ÿè¡Œã—ã¦ãã ã•ã„")
        
        X = self._validate_input(X)
        
        if len(X) == 0:
            return np.array([])
        
        X_2d = X.reshape(-1, 1)
        
        # Step 1: æ­£è¦åˆ†å¸ƒã¸å¤‰æ›
        z_scores = self.qt.transform(X_2d)
        
        # Step 2: ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
        scaled_scores = z_scores * self.scale_factor
        
        # Step 3: ç¯„å›²åˆ¶é™
        final_scores = np.clip(scaled_scores, self.target_range[0], self.target_range[1])
        
        return final_scores.flatten()
    
    def fit_transform(self, X: np.ndarray) -> np.ndarray:
        """fit ã¨ transform ã‚’ä¸€åº¦ã«å®Ÿè¡Œ"""
        return self.fit(X).transform(X)
    
    def save(self, filepath: str):
        """å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜"""
        if not self.is_fitted:
            raise RuntimeError("fit() ã‚’å…ˆã«å®Ÿè¡Œã—ã¦ãã ã•ã„")
        
        os.makedirs(os.path.dirname(filepath), exist_ok=True)
        
        joblib.dump(self, filepath)
        logger.info(f"ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {filepath}")
    
    @classmethod
    def load(cls, filepath: str) -> 'RacingIndexNormalizer':
        """å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
        if not os.path.exists(filepath):
            raise FileNotFoundError(f"ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {filepath}")
        
        normalizer = joblib.load(filepath)
        logger.info(f"ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ: {filepath}")
        
        return normalizer
    
    def _validate_input(self, X: np.ndarray) -> np.ndarray:
        """å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼ã¨å¤‰æ›"""
        if not isinstance(X, np.ndarray):
            X = np.array(X)
        
        if X.ndim == 2 and X.shape[1] == 1:
            X = X.flatten()
        elif X.ndim > 1:
            raise ValueError(f"å…¥åŠ›ã¯1æ¬¡å…ƒé…åˆ—ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™: shape={X.shape}")
        
        if np.any(np.isnan(X)):
            logger.warning(f"NaNãŒå«ã¾ã‚Œã¦ã„ã¾ã™: {np.sum(np.isnan(X))}ä»¶")
        
        return X
    
    def _log_statistics(self, X: np.ndarray):
        """å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®çµ±è¨ˆæƒ…å ±ã‚’ãƒ­ã‚°å‡ºåŠ›"""
        logger.info("=== å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®çµ±è¨ˆæƒ…å ± ===")
        logger.info(f"ãƒ‡ãƒ¼ã‚¿æ•°: {len(X):,}ä»¶")
        logger.info(f"æœ€å°å€¤: {np.min(X):.2f}")
        logger.info(f"æœ€å¤§å€¤: {np.max(X):.2f}")
        logger.info(f"å¹³å‡å€¤: {np.mean(X):.2f}")
        logger.info(f"ä¸­å¤®å€¤: {np.median(X):.2f}")
        logger.info(f"æ¨™æº–åå·®: {np.std(X):.2f}")
        logger.info(f"5%ç‚¹: {np.percentile(X, 5):.2f}")
        logger.info(f"25%ç‚¹: {np.percentile(X, 25):.2f}")
        logger.info(f"75%ç‚¹: {np.percentile(X, 75):.2f}")
        logger.info(f"95%ç‚¹: {np.percentile(X, 95):.2f}")
        logger.info("=" * 30)


# ============================
# å‰åŠ3Fæ¨å®šãƒ­ã‚¸ãƒƒã‚¯ï¼ˆåŸ‹ã‚è¾¼ã¿ç‰ˆï¼‰
# ============================

# è·é›¢æ¯”ç‡å®šç¾©ï¼ˆten_3f_estimator.py ã‚ˆã‚Šï¼‰
DISTANCE_RATIOS = {
    1200: None,  # 1200m ã¯ç‰¹åˆ¥å‡¦ç†ï¼ˆèµ°ç ´ã‚¿ã‚¤ãƒ  - å¾ŒåŠ3Fï¼‰
    1230: 0.495,
    1300: 0.48,
    1400: 0.26,
    1500: 0.24,
    1600: 0.22,
    1700: 0.21,
    1800: 0.22,
    2000: 0.17,
    2100: 0.16
}

MIN_TEN_3F = 30.0  # å‰åŠ3F ã®ç‰©ç†çš„ä¸‹é™
MAX_TEN_3F = 45.0  # å‰åŠ3F ã®ç‰©ç†çš„ä¸Šé™


def get_distance_ratio(kyori: int) -> float:
    """
    è·é›¢ã‹ã‚‰å‰åŠ3Fã®æ¯”ç‡ã‚’å–å¾—ï¼ˆè£œé–“ã‚ã‚Šï¼‰
    
    Args:
        kyori: è·é›¢ï¼ˆmï¼‰
    
    Returns:
        å‰åŠ3Fã®æ¯”ç‡
    """
    # å®Œå…¨ä¸€è‡´
    if kyori in DISTANCE_RATIOS and DISTANCE_RATIOS[kyori] is not None:
        return DISTANCE_RATIOS[kyori]
    
    # 1200m ã¯ç‰¹åˆ¥å‡¦ç†
    if kyori <= 1200:
        return 0.50  # 1200mä»¥ä¸‹ã¯50%
    
    # ç·šå½¢è£œé–“
    sorted_distances = sorted([k for k in DISTANCE_RATIOS.keys() if k > 1200 and DISTANCE_RATIOS[k] is not None])
    
    for i in range(len(sorted_distances) - 1):
        d1 = sorted_distances[i]
        d2 = sorted_distances[i + 1]
        
        if d1 <= kyori <= d2:
            r1 = DISTANCE_RATIOS[d1]
            r2 = DISTANCE_RATIOS[d2]
            # ç·šå½¢è£œé–“
            ratio = r1 + (r2 - r1) * (kyori - d1) / (d2 - d1)
            return ratio
    
    # ç¯„å›²å¤–ã®å ´åˆ
    if kyori > max(sorted_distances):
        return 0.15  # 2100mè¶…ã¯15%
    
    return 0.22  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ


def estimate_zenhan_3f(soha_time_sec: float, kohan_3f_sec: float, kyori: int) -> float:
    """
    å‰åŠ3Fã‚’æ¨å®šï¼ˆ3ãƒ‘ã‚¿ãƒ¼ãƒ³å¯¾å¿œï¼‰
    
    Args:
        soha_time_sec: èµ°ç ´ã‚¿ã‚¤ãƒ ï¼ˆç§’ï¼‰
        kohan_3f_sec: å¾ŒåŠ3Fï¼ˆç§’ï¼‰
        kyori: è·é›¢ï¼ˆmï¼‰
    
    Returns:
        æ¨å®šå‰åŠ3Fï¼ˆç§’ï¼‰
    """
    # ãƒ‘ã‚¿ãƒ¼ãƒ³1: 1200mæœªæº€ â†’ èµ°ç ´ã‚¿ã‚¤ãƒ  - å¾ŒåŠ3F
    if kyori < 1200:
        zenhan_3f = soha_time_sec - kohan_3f_sec
        return max(MIN_TEN_3F, min(MAX_TEN_3F, zenhan_3f))
    
    # ãƒ‘ã‚¿ãƒ¼ãƒ³2: 1200m ã¡ã‚‡ã†ã© â†’ èµ°ç ´ã‚¿ã‚¤ãƒ  - å¾ŒåŠ3F
    if kyori == 1200:
        zenhan_3f = soha_time_sec - kohan_3f_sec
        return max(MIN_TEN_3F, min(MAX_TEN_3F, zenhan_3f))
    
    # ãƒ‘ã‚¿ãƒ¼ãƒ³3: 1201mä»¥ä¸Š â†’ è·é›¢æ¯”ç‡ã‚’ä½¿ç”¨
    ratio = get_distance_ratio(kyori)
    zenhan_3f = soha_time_sec * ratio
    
    # ç‰©ç†çš„åˆ¶ç´„ï¼ˆ30.0 ~ 45.0ç§’ï¼‰
    zenhan_3f = max(MIN_TEN_3F, min(MAX_TEN_3F, zenhan_3f))
    
    return zenhan_3f


# ============================
# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨å‰å‡¦ç†
# ============================

def load_and_filter_data(
    file_path: str,
    start_date: str = '20231013',
    end_date: str = '20251231',
    sample_rate: float = 1.0
) -> pd.DataFrame:
    """
    CSVãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã€ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    
    Args:
        file_path: ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        start_date: é–‹å§‹æ—¥ï¼ˆYYYYMMDDï¼‰
        end_date: çµ‚äº†æ—¥ï¼ˆYYYYMMDDï¼‰
        sample_rate: ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ç‡ï¼ˆ0.0-1.0ã€1.0=å…¨ãƒ‡ãƒ¼ã‚¿ï¼‰
    
    Returns:
        ãƒ•ã‚£ãƒ«ã‚¿æ¸ˆã¿DataFrame
    """
    logger.info(f"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿é–‹å§‹: {file_path}")
    
    # å¿…è¦ãªåˆ—ã®ã¿èª­ã¿è¾¼ã¿
    required_cols = [
        'race_id', 'umaban', 'chakujun', 'tosu',
        'soha_time_sec', 'kohan_3f_sec', 'kyori',
        'race_date', 'keibajo_code', 'wakuban', 'weight_kg',
        'corner_1', 'corner_2', 'corner_3', 'corner_4'
    ]
    
    # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°èª­ã¿è¾¼ã¿
    if sample_rate < 1.0:
        df = pd.read_csv(
            file_path,
            usecols=lambda col: col in required_cols,
            skiprows=lambda i: i > 0 and np.random.random() > sample_rate
        )
    else:
        df = pd.read_csv(file_path, usecols=required_cols)
    
    logger.info(f"èª­ã¿è¾¼ã¿å®Œäº†: {len(df):,}è¡Œ")
    
    # æœŸé–“ãƒ•ã‚£ãƒ«ã‚¿
    df = df[(df['race_date'] >= int(start_date)) & (df['race_date'] <= int(end_date))]
    logger.info(f"æœŸé–“ãƒ•ã‚£ãƒ«ã‚¿å¾Œ: {len(df):,}è¡Œ")
    
    # ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°
    df = df.dropna(subset=['race_id', 'soha_time_sec', 'kohan_3f_sec', 'kyori', 'tosu'])
    df = df[df['soha_time_sec'] > 0]
    df = df[df['kohan_3f_sec'] > 0]
    df = df[df['tosu'] >= 4]
    
    logger.info(f"ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°å¾Œ: {len(df):,}è¡Œ")
    
    return df


# ============================
# æŒ‡æ•°è¨ˆç®—ï¼ˆå®Ÿè£…ç‰ˆãƒ­ã‚¸ãƒƒã‚¯ï¼‰
# ============================

def calculate_all_indices(df: pd.DataFrame) -> pd.DataFrame:
    """
    å…¨æŒ‡æ•°ã‚’è¨ˆç®—
    
    Args:
        df: å…¥åŠ›DataFrame
    
    Returns:
        æŒ‡æ•°åˆ—ãŒè¿½åŠ ã•ã‚ŒãŸDataFrame
    """
    logger.info("æŒ‡æ•°è¨ˆç®—é–‹å§‹...")
    
    results = []
    
    for idx, row in df.iterrows():
        try:
            race_id = row['race_id']
            umaban = row['umaban']
            chakujun = row['chakujun']
            tosu = int(row['tosu'])
            soha_time_sec = float(row['soha_time_sec'])
            kohan_3f_sec = float(row['kohan_3f_sec'])
            kyori = int(row['kyori'])
            
            # å‰åŠ3Fæ¨å®šï¼ˆ3ãƒ‘ã‚¿ãƒ¼ãƒ³å¯¾å¿œï¼‰
            zenhan_3f = estimate_zenhan_3f(soha_time_sec, kohan_3f_sec, kyori)
            
            # ã‚³ãƒ¼ãƒŠãƒ¼é †ä½ï¼ˆcorner_4ã‚’å„ªå…ˆï¼‰
            if 'corner_4' in row and pd.notna(row['corner_4']):
                corner_4 = int(row['corner_4'])
            else:
                corner_4 = int(row['chakujun']) if 'chakujun' in row else tosu // 2
            
            # åŸºæº–ã‚¿ã‚¤ãƒ ï¼ˆç°¡æ˜“ç‰ˆï¼šè·é›¢ã®ã¿è€ƒæ…®ï¼‰
            if kyori <= 1200:
                base_time = 37.5
            elif kyori <= 1400:
                base_time = 38.0
            elif kyori <= 1600:
                base_time = 39.0
            elif kyori <= 1800:
                base_time = 39.5
            elif kyori <= 2000:
                base_time = 40.0
            else:
                base_time = 40.5
            
            # 1. ä¸ŠãŒã‚ŠæŒ‡æ•°ï¼ˆÃ—1ã€è£œæ­£ãªã—ç°¡æ˜“ç‰ˆï¼‰
            agari_index = (base_time - kohan_3f_sec)
            
            # 2. ä½ç½®æŒ‡æ•°ï¼ˆ0ï½100ã€ã‚³ãƒ¼ãƒŠãƒ¼4è§’ãƒ™ãƒ¼ã‚¹ï¼‰
            avg_position = corner_4
            base_position = tosu / 2.0
            position_index = ((base_position - avg_position) / tosu) * 100
            position_index = max(0, min(100, position_index))
            
            # 3. ãƒ†ãƒ³æŒ‡æ•°ï¼ˆÃ—1ã€è£œæ­£ãªã—ç°¡æ˜“ç‰ˆï¼‰
            ten_index = (base_time - zenhan_3f)
            
            # 4. ãƒšãƒ¼ã‚¹æŒ‡æ•°ï¼ˆå¹³å‡ï¼‰
            pace_index = (ten_index + agari_index) / 2
            
            results.append({
                'race_id': race_id,
                'umaban': umaban,
                'chakujun': chakujun,
                'tosu': tosu,
                'ä¸ŠãŒã‚ŠæŒ‡æ•°': agari_index,
                'ä½ç½®æŒ‡æ•°': position_index,
                'ãƒ†ãƒ³æŒ‡æ•°': ten_index,
                'ãƒšãƒ¼ã‚¹æŒ‡æ•°': pace_index
            })
            
        except Exception as e:
            logger.warning(f"æŒ‡æ•°è¨ˆç®—ã‚¨ãƒ©ãƒ¼ (è¡Œ {idx}): {e}")
            continue
    
    result_df = pd.DataFrame(results)
    logger.info(f"æŒ‡æ•°è¨ˆç®—å®Œäº†: {len(result_df):,}é ­")
    
    return result_df


# ============================
# åˆ†å¸ƒåˆ†æ
# ============================

def analyze_distribution(data: np.ndarray, index_name: str):
    """
    æŒ‡æ•°ã®åˆ†å¸ƒã‚’åˆ†æã—ã¦ãƒ­ã‚°å‡ºåŠ›
    
    Args:
        data: æŒ‡æ•°ãƒ‡ãƒ¼ã‚¿
        index_name: æŒ‡æ•°å
    """
    logger.info(f"\n{'='*50}")
    logger.info(f"ğŸ“Š {index_name} ã®åˆ†å¸ƒåˆ†æ")
    logger.info(f"{'='*50}")
    
    logger.info(f"ãƒ‡ãƒ¼ã‚¿æ•°: {len(data):,}ä»¶")
    logger.info(f"æœ€å°å€¤: {np.min(data):.2f}")
    logger.info(f"æœ€å¤§å€¤: {np.max(data):.2f}")
    logger.info(f"å¹³å‡å€¤: {np.mean(data):.2f}")
    logger.info(f"ä¸­å¤®å€¤: {np.median(data):.2f}")
    logger.info(f"æ¨™æº–åå·®: {np.std(data):.2f}")
    
    # åˆ†ä½ç‚¹
    logger.info("\nğŸ“ˆ åˆ†ä½ç‚¹:")
    logger.info(f"  5%ç‚¹: {np.percentile(data, 5):.2f}")
    logger.info(f"  25%ç‚¹: {np.percentile(data, 25):.2f}")
    logger.info(f"  50%ç‚¹ (ä¸­å¤®å€¤): {np.percentile(data, 50):.2f}")
    logger.info(f"  75%ç‚¹: {np.percentile(data, 75):.2f}")
    logger.info(f"  95%ç‚¹: {np.percentile(data, 95):.2f}")
    
    # ç¯„å›²åˆ¥ã®é›†ä¸­åº¦ãƒã‚§ãƒƒã‚¯
    if index_name in ['ãƒ†ãƒ³æŒ‡æ•°', 'ä¸ŠãŒã‚ŠæŒ‡æ•°', 'ãƒšãƒ¼ã‚¹æŒ‡æ•°']:
        min_range = -100
        max_range = 100
        bins = [-100, -90, -50, -10, 10, 50, 90, 100]
    else:  # ä½ç½®æŒ‡æ•°
        min_range = 0
        max_range = 100
        bins = [0, 5, 25, 50, 75, 95, 100]
    
    logger.info("\nğŸ“Š åŒºé–“åˆ¥åˆ†å¸ƒ:")
    for i in range(len(bins) - 1):
        count = np.sum((data >= bins[i]) & (data < bins[i+1]))
        pct = count / len(data) * 100
        logger.info(f"  [{bins[i]:5.0f} ~ {bins[i+1]:5.0f}): {count:7,}ä»¶ ({pct:5.2f}%)")
    
    # æœ€å°å€¤/æœ€å¤§å€¤ã¸ã®å¼µã‚Šä»˜ããƒã‚§ãƒƒã‚¯
    min_count = np.sum(data == min_range)
    max_count = np.sum(data == max_range)
    
    if min_count > 0:
        min_pct = min_count / len(data) * 100
        logger.info(f"\nâš ï¸  æœ€å°å€¤ {min_range} ã¸ã®å¼µã‚Šä»˜ã: {min_count:,}ä»¶ ({min_pct:.2f}%)")
    
    if max_count > 0:
        max_pct = max_count / len(data) * 100
        logger.info(f"âš ï¸  æœ€å¤§å€¤ {max_range} ã¸ã®å¼µã‚Šä»˜ã: {max_count:,}ä»¶ ({max_pct:.2f}%)")


# ============================
# ãƒ¡ã‚¤ãƒ³å‡¦ç†
# ============================

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    logger.info("="*60)
    logger.info("ğŸ‡ NAR-SI3.0 æŒ‡æ•°æ­£è¦åŒ–å™¨ å­¦ç¿’ãƒ—ãƒ­ã‚°ãƒ©ãƒ ï¼ˆWindowsç‰ˆï¼‰")
    logger.info("="*60)
    
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š
    DEFAULT_DATA_PATH = r'E:\UmaData\nar-analytics-python-v2\data-1768047611955.csv'
    DEFAULT_START_DATE = '20231013'
    DEFAULT_END_DATE = '20251231'
    DEFAULT_SAMPLE_RATE = 1.0  # å…¨ãƒ‡ãƒ¼ã‚¿ä½¿ç”¨
    DEFAULT_OUTPUT_DIR = 'models/normalizers'
    
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›
    print("\n" + "="*60)
    print("âš™ï¸  è¨­å®šå…¥åŠ›ï¼ˆEnter ã§ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä½¿ç”¨ï¼‰")
    print("="*60)
    
    data_path = input(f"ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ [{DEFAULT_DATA_PATH}]: ").strip()
    data_path = data_path if data_path else DEFAULT_DATA_PATH
    
    start_date = input(f"é–‹å§‹æ—¥ (YYYYMMDD) [{DEFAULT_START_DATE}]: ").strip()
    start_date = start_date if start_date else DEFAULT_START_DATE
    
    end_date = input(f"çµ‚äº†æ—¥ (YYYYMMDD) [{DEFAULT_END_DATE}]: ").strip()
    end_date = end_date if end_date else DEFAULT_END_DATE
    
    sample_rate_input = input(f"ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ç‡ (0.0-1.0) [{DEFAULT_SAMPLE_RATE}]: ").strip()
    sample_rate = float(sample_rate_input) if sample_rate_input else DEFAULT_SAMPLE_RATE
    
    output_dir = input(f"å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª [{DEFAULT_OUTPUT_DIR}]: ").strip()
    output_dir = output_dir if output_dir else DEFAULT_OUTPUT_DIR
    
    # ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ãƒã‚§ãƒƒã‚¯
    if not os.path.exists(data_path):
        logger.error(f"âŒ ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {data_path}")
        return
    
    # ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨å‰å‡¦ç†
    logger.info("\n" + "="*60)
    logger.info("ğŸ“‚ ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨å‰å‡¦ç†")
    logger.info("="*60)
    
    df = load_and_filter_data(data_path, start_date, end_date, sample_rate)
    
    if len(df) == 0:
        logger.error("âŒ æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
        return
    
    # ã‚¹ãƒ†ãƒƒãƒ—2: æŒ‡æ•°è¨ˆç®—
    logger.info("\n" + "="*60)
    logger.info("ğŸ”¢ ã‚¹ãƒ†ãƒƒãƒ—2: æŒ‡æ•°è¨ˆç®—ï¼ˆå®Ÿè£…ç‰ˆãƒ­ã‚¸ãƒƒã‚¯ï¼‰")
    logger.info("="*60)
    
    index_df = calculate_all_indices(df)
    
    if len(index_df) == 0:
        logger.error("âŒ æŒ‡æ•°è¨ˆç®—ã«å¤±æ•—ã—ã¾ã—ãŸ")
        return
    
    # ã‚¹ãƒ†ãƒƒãƒ—3: æ­£è¦åŒ–å‰ã®åˆ†å¸ƒåˆ†æ
    logger.info("\n" + "="*60)
    logger.info("ğŸ“Š ã‚¹ãƒ†ãƒƒãƒ—3: æ­£è¦åŒ–å‰ã®åˆ†å¸ƒåˆ†æ")
    logger.info("="*60)
    
    analyze_distribution(index_df['ãƒ†ãƒ³æŒ‡æ•°'].values, 'ãƒ†ãƒ³æŒ‡æ•°')
    analyze_distribution(index_df['ä¸ŠãŒã‚ŠæŒ‡æ•°'].values, 'ä¸ŠãŒã‚ŠæŒ‡æ•°')
    analyze_distribution(index_df['ä½ç½®æŒ‡æ•°'].values, 'ä½ç½®æŒ‡æ•°')
    analyze_distribution(index_df['ãƒšãƒ¼ã‚¹æŒ‡æ•°'].values, 'ãƒšãƒ¼ã‚¹æŒ‡æ•°')
    
    # ã‚¹ãƒ†ãƒƒãƒ—4: æ­£è¦åŒ–å™¨ã®å­¦ç¿’
    logger.info("\n" + "="*60)
    logger.info("ğŸ“ ã‚¹ãƒ†ãƒƒãƒ—4: æ­£è¦åŒ–å™¨ã®å­¦ç¿’")
    logger.info("="*60)
    
    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    os.makedirs(output_dir, exist_ok=True)
    
    normalizers = {}
    
    # ãƒ†ãƒ³æŒ‡æ•°
    logger.info("\nğŸ”¹ ãƒ†ãƒ³æŒ‡æ•°ã®æ­£è¦åŒ–å™¨ã‚’å­¦ç¿’ä¸­...")
    ten_normalizer = RacingIndexNormalizer()
    ten_normalizer.fit(index_df['ãƒ†ãƒ³æŒ‡æ•°'].values)
    ten_path = os.path.join(output_dir, 'ten_index_normalizer.pkl')
    ten_normalizer.save(ten_path)
    normalizers['ten'] = ten_normalizer
    
    # ä¸ŠãŒã‚ŠæŒ‡æ•°
    logger.info("\nğŸ”¹ ä¸ŠãŒã‚ŠæŒ‡æ•°ã®æ­£è¦åŒ–å™¨ã‚’å­¦ç¿’ä¸­...")
    agari_normalizer = RacingIndexNormalizer()
    agari_normalizer.fit(index_df['ä¸ŠãŒã‚ŠæŒ‡æ•°'].values)
    agari_path = os.path.join(output_dir, 'agari_index_normalizer.pkl')
    agari_normalizer.save(agari_path)
    normalizers['agari'] = agari_normalizer
    
    # ä½ç½®æŒ‡æ•°ï¼ˆ0ï½100ï¼‰
    logger.info("\nğŸ”¹ ä½ç½®æŒ‡æ•°ã®æ­£è¦åŒ–å™¨ã‚’å­¦ç¿’ä¸­...")
    position_normalizer = RacingIndexNormalizer(target_range=(0, 100))
    position_normalizer.fit(index_df['ä½ç½®æŒ‡æ•°'].values)
    position_path = os.path.join(output_dir, 'position_index_normalizer.pkl')
    position_normalizer.save(position_path)
    normalizers['position'] = position_normalizer
    
    # ãƒšãƒ¼ã‚¹æŒ‡æ•°
    logger.info("\nğŸ”¹ ãƒšãƒ¼ã‚¹æŒ‡æ•°ã®æ­£è¦åŒ–å™¨ã‚’å­¦ç¿’ä¸­...")
    pace_normalizer = RacingIndexNormalizer()
    pace_normalizer.fit(index_df['ãƒšãƒ¼ã‚¹æŒ‡æ•°'].values)
    pace_path = os.path.join(output_dir, 'pace_index_normalizer.pkl')
    pace_normalizer.save(pace_path)
    normalizers['pace'] = pace_normalizer
    
    # ã‚¹ãƒ†ãƒƒãƒ—5: æ­£è¦åŒ–å¾Œã®åˆ†å¸ƒç¢ºèª
    logger.info("\n" + "="*60)
    logger.info("ğŸ“Š ã‚¹ãƒ†ãƒƒãƒ—5: æ­£è¦åŒ–å¾Œã®åˆ†å¸ƒç¢ºèª")
    logger.info("="*60)
    
    ten_normalized = ten_normalizer.transform(index_df['ãƒ†ãƒ³æŒ‡æ•°'].values)
    agari_normalized = agari_normalizer.transform(index_df['ä¸ŠãŒã‚ŠæŒ‡æ•°'].values)
    position_normalized = position_normalizer.transform(index_df['ä½ç½®æŒ‡æ•°'].values)
    pace_normalized = pace_normalizer.transform(index_df['ãƒšãƒ¼ã‚¹æŒ‡æ•°'].values)
    
    analyze_distribution(ten_normalized, 'ãƒ†ãƒ³æŒ‡æ•°ï¼ˆæ­£è¦åŒ–å¾Œï¼‰')
    analyze_distribution(agari_normalized, 'ä¸ŠãŒã‚ŠæŒ‡æ•°ï¼ˆæ­£è¦åŒ–å¾Œï¼‰')
    analyze_distribution(position_normalized, 'ä½ç½®æŒ‡æ•°ï¼ˆæ­£è¦åŒ–å¾Œï¼‰')
    analyze_distribution(pace_normalized, 'ãƒšãƒ¼ã‚¹æŒ‡æ•°ï¼ˆæ­£è¦åŒ–å¾Œï¼‰')
    
    # å®Œäº†ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
    logger.info("\n" + "="*60)
    logger.info("âœ… å…¨ã¦ã®æ­£è¦åŒ–å™¨ã®å­¦ç¿’ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
    logger.info("="*60)
    logger.info(f"\nğŸ“ ä¿å­˜å…ˆ: {output_dir}")
    logger.info(f"  - {ten_path}")
    logger.info(f"  - {agari_path}")
    logger.info(f"  - {position_path}")
    logger.info(f"  - {pace_path}")
    
    logger.info("\nğŸ¯ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
    logger.info("  1. ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèª")
    logger.info("  2. äºˆæ¸¬ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã«çµ±åˆ")
    logger.info("  3. å®Ÿæˆ¦ã§ã®åŠ¹æœã‚’æ¤œè¨¼")
    
    logger.info("\nğŸ† Play to Win!")


if __name__ == '__main__':
    try:
        main()
    except KeyboardInterrupt:
        logger.info("\nâš ï¸  å‡¦ç†ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
    except Exception as e:
        logger.error(f"\nâŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback
        traceback.print_exc()
